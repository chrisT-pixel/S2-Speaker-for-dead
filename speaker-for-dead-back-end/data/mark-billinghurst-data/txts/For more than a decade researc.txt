For more than a decade researchers have tried to create intuitive computer interfaces by blending reali
ty and virtual reality. The goal is for people to interact with the digital domain as easily as with the real world. Various approaches help us achieve thisin the area of tangible interfaces, we use real objects as interface widgets; in augmented reality, researchers overlay 3D virtual imagery onto the real world; and in VR interfaces, we entirely replace the real world with a computergenerated environment. 
As Milgram pointed out,1 these types of computer interfaces can be placed along a continuum according to how much of the users environment is computer generated (Figure 1). Tangible interfaces lie far to the left on this realityvirtuality line, while immersive virtual environments are at the right extremity. Most current user interfaces exist as discrete points along this continuum. 
However, human activity cant always be broken into discrete components and for many tasks users may prefer to move seamlessly along the realityvirtuality continuum. This proves true when interacting with 3D graphical content, either creating virtual models or viewing them. For example, if people want to experience a virtual scene from different scales, then immersive virtual reality may be ideal. If they want to have a facetoface discussion while viewing the virtual scene, an augmented reality interface may be best.2 The MagicBook project is an early attempt to explore how we can use a physical object to smoothly transport users between reality and virtuality. 
pages of a fairy tale and becoming part of the story. The MagicBook project makes this fantasy a reality using a normal book as the main interface object. People can turn the pages of the book, look at the pictures, and read 
the text without any additional technology (Figure 2a). However, if a person looks at the pages through an augmented reality display, they see 3D virtual models appearing out of the pages (Figure 2b). The models appear attached to the real page so users can see the augmented reality scene from any perspective by moving themselves or the book. The virtual content can be any size and is animated, so the augmented reality view is an enhanced version of a traditional 3D popup book. 
Users can change the virtual models by turning the book pages. When they see a scene they particularly like, they can fly into the page and experience the story as an immersive virtual environment (Figure 2c). In the VR view, theyre free to move about the scene at will, so using the MagicBook people can experience the full realityvirtuality continuum. 
Real books often serve as the focus for facetoface collaboration and in a similar way multiple people can use the MagicBook interface at the same time. Several readers can look at the same book and share the story together. If theyre using the augmented reality displays, they can each see the virtual models from their own viewpoint. Since they can see each other at the same time as the virtual models, they can easily communicate using normal facetoface conversation cues. 
Multiple users can immerse in the same virtual scene where theyll see each other represented as virtual characters (Figure 3a). More interestingly, one or more people may immerse themselves in the virtual world while others view the content as an augmented reality scene. In this case, those viewing the augmented reality scene will see a miniature avatar of the immersive user in the virtual world (Figure 3b). In the immersive world, people viewing the augmented reality scene appear as large, virtual heads looking down from the sky. This way, people are always aware of the other users of the interface and where they are looking. 
As an augmented reality object. Users with augmented reality displays can see virtual objects appearing on the pages of the book from their own viewpoint. 
As an immersive virtual space. Users can fly into the virtual space together and see each other represented as virtual avatars in the story space. 
mented reality display (HHD), a computer graphics workstation, and the physical book. Users have their own handheld display and computer to generate their individual scene views. These computers are networked together to exchange information about avatar positions and the virtual scene each user views. The HHD is a handle with a Sony Glasstron PLMA35 display mounted at the top, an InterSense InterTrax inertial tracker at the bottom, a small color video camera on the front of the Glasstron display, and a switch and pressure pad embedded in the handle (Figure 4). The PLMA35 is a lowcost bioccular display with two liquid crystal display (LCD) panels of 266 225 pixel resolution. 
The camera output connects to the computer graphics workstation. Computer graphics overlay video of the real world and the resulting composite image is shown in the Glasstron display. In this way, users experience the real world as a videomediated reality. 
The books used in the MagicBook interface are normal books with text and pictures on each page. Certain pictures have thick, black borders that are used as tracking marks for a computer visionbased headtracking system. When the reader looks at these pictures through the handheld display, computer vision techniques precisely calculate the camera position and orientation relative to the tracking mark. The head tracking uses the augmented reality tool kit (ARToolKit) tracking library, an opensource software library for developing visionbased augmented reality applications (see the ARToolKit Web site, http://www.hitl.washington.edu/research/shared_ space/download/). Once the computer calculates the users head position, it generates virtual images that appear precisely registered with the real pages. The MagicBook application can track the users head position and render graphics at 30 frames per second on a Pentium III 800MHz computer. 
When users see an augmented reality scene they wish to explore, flicking the switch on the handle will transition them into the immersive environment. The real world is no longer visible, so the head tracking changes from the computervision module to the InterTrax inertial
2 Using the MagicBook interface to move between reality and virtual reality. (a) Reality, (b) augmented reality, and (c) immersive VR. 
3 Collaboration using the MagicBook. (a) Avatar in an immersive scene or (b) in an exocentric augmented reality view. 
orientation tracker. Readers can look around the scene in any direction. By pushing the pressure pad on the handle, they can fly in the direction theyre looking. The harder they push, the faster they fly. To return to the real world, users flick the switch again. 
The Opera glass form factor of the handheld display encourage seamless transistion between reality and virtual reality. Users can look through the display to see augmented reality and VR content but can return to viewing the real world by removing the display from their eyes. The handheld display is also easy to share, enabling several people to try a single display unit and see the same content as they pass it among themselves. 
Virtual Reality Modeling Language (VRML) 97 parser. Content developers can use the parser to show almost any virtual imagery using this interface. Weve created nearly a dozen books in a variety of application domainsincluding architecture, scientific visualization, education, and entertainment. Once the virtual content is created, its easy to make the physical book pages, train the computer vision system to recognize the particular tracking markers on each page, and update the configuration files to load the correct content. 
The low cost and ease of use makes the MagicBook interface an attractive means of viewing almost any spatial data. Those applications that involve moving between exocentric and egocentric views of a data set could benefit the most from this approach. 
we need new interfaces that blur the line between reality and virtuality and let users easily move between the physical and digital domains. The MagicBook is an early attempt at a transitional interface for viewing spatial data sets. In addition, the MagicBook supports collaboration on multiple levels. 
Although initial user feedback has been positive, we continue to improve the interface. In the future, we plan 
on exploring more intuitive ways for users to navigate through and interact with the virtual models. Were also working on integrating the MagicBook approach into an environment with projective displays, which will allow a seamless transition between 2D and 3D views of a data set in a traditional office setting. 
For more information about this project and a copy of the ARToolKit software, please visit http://www.hitl. washington.edu/magicbook/. 
Communication Research Laboratories (MIC Lab) of Advanced Telecommunications Research Institute International (ATR), Japan, for their continued support, and also Keiko Nakao, Susan Campbell, and Dace Campbell for making the models and books in this article. 
Reality Visual Displays, Institute of Electronics, Information, and Communication Engineers Trans. Information and Systems (IECE special issue on networked reality), vol. E77D, no. 12, 1994, pp.13211329. 
2. K. Kiyokawa et al., Collaborative Immersive Workspace through a Shared Augmented Environment, Proc. Intl Soc. for Optical Eng. (SPIE 98), SPIE Press, Bellingham, Wash., vol. 3517, 1998, pp. 213. 
Readers may contact Billinghurst at the Human Interface Technology Laboratory, Univ. of Washington, PO Box 352142, Seattle, WA 98195, email grof@hitl.washington. edu. 
Readers may contact the department editors by email at rosenblum@ait.nrl.navy.mil or Michael_Macedonia@ stricom.army.mil. 
