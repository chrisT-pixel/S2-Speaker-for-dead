An Evaluation of Wearable Information Spaces M. Billinghurst, J. Bowskill, Nick Dyer, Jason Morphett 
Human Interface Technology Laboratory Advanced Perception Unit University of Washington BT Laboratories Box 352142 Martlesham Heath Seattle, WA 98195 Ipswich, IP5 3RE USA United Kingdom grof@hitl.washington.edu , {jerry.bowskill, nick.dyer, jason.morphett}@btsys.bt.co.uk 
ABSTRACT Two dimensional windows based interfaces may not be appropriate for wearable computers. In this paper we draw on established virtual reality techniques to design and evaluate several alternate methods for information presentation in a wearable environment. We find simple bodyspatialised displays provide benefits over traditional headstabilised displays. Users found the bodystabilised displays easier to use, more enjoyable and more intuitive, and were able to perform significantly better on a search task. Spatial audio and visual cues further enhanced performance. 
INTRODUCTION One of the broad trends emerging in advanced humancomputer interaction is the increasing portability of computing power. Wearable computers are the next generation of portable machines. Worn on the body they provide constant access to computing and communications resources. However, for wearable computing to be widely adopted there are unique interface challenges that need to be solved. One of the most important issues is how to present and interact with information in a wearable environment. In this paper we apply traditional virtual reality techniques to develop and evaluate spatialised interfaces for wearables and present results comparing user performance with these interface to a more traditional wearable interface. 
BACKGROUND The field of wearable computing encompasses a very wide range of devices. In general, a wearable computer may be defined as a computer that is subsumed into the personal space of the user, controlled by the wearer and has both operational and interactional constancy, i.e. is always on and always accessible [1]. Wearables are typically composed of a belt or back pack PC, head mounted display (HMD), wireless communications hardware and an input device such as touchpad or chording keyboard. This configuration has been demonstrated in a number of real world applications including aircraft mantainence [2], navigational assistance [3] and vehicle mechanics 
[4]. In such applications wearables have dramatically improved user performance, reducing task time by half in the case of vehicle inspection [4]. 
There are unique challenges in designing interfaces for wearable computers. Although most current wearable applications use traditional twodimensional GUIs, these interfaces have been optimised for desktop use and are less than ideal for the wearable platform, both because of the nature of the tasks wearables are used for and the unique input and output devices wearables have. For example, wearable input devices must be able to be used with one hand, when out of view, and at an arbitrary orientation. Previous researchers have used speech [2], one handed twiddlers [5], half keyboards [6] and dials [4] in wearable interfaces. Less work has been done on the graphical interface. Wearables predominantly have small monoscopic head mounted displays with limited resolution and a narrow field of view. We are interested in developing interface metaphors that are ideally suited for head mounted displays on a wearable computer. Wearables are currently most commonly used for data access and display so we have initially focussed on the problem of 2D information presentation using a monoscopic display. In this paper we show how spatial display techniques can be used to improve the display of 2D informaiton in a wearable computing environment. 
SPATIAL INFORMATION DISPLAY Information presentation using a head mounted display has been well studied in the virtual reality arena. Since most wearable displays are seethrough or seearound displays, augmented reality interfaces are most relevant to our work. In this setting information can be presented in a combination of three ways: 
Headstabilisedinformation is fixed to the users viewpoint and doesnt change as the user changes viewpoint orientation or position. Bodystabilisedinformation is fixed relative to the users body position and varies as the user changes viewpoint orientation, but not as they change position. Worldstabilisedinformation is fixed to real world locations and varies as the user changes viewpoint orientation and position.
Each of these presentation methods require increasingly complex head tracking technologies, as shown in table 1.0. The registration requirements also become more difficult progressing from head to world stabilised images; no registration is required for head stabilised images, while complex calibration techniques are required to achieve good world stabilisation [7]. 
Information Presentation Tracking Required head stabilised None body stabilised Orientation world stabilised Position and Orientation 
Body and World stabilised information display is attractive for a number of reasons. As Reichlen[8] demonstrates, a bodystabilised information space can overcome the resolution limitations of head mounted displays. In his work a user wears a head mounted display while seated on a rotatable chair. By tracking head orientation the user experiences a hemispherical information surroundin effect a hundred million pixel display. Worldstabilised information allows annotating the real world with context dependent data and creating information enriched environments [9]. This increases the intuitiveness of real world tasks. For example, Rekimoto uses worldstabilised virtual tags to label parts of real world objects [10] while researchers at the University of North Carolina register virtual fetal ultrasound views on the womb[11]. In general spatial information displays enable humans to use their innate spatial abilities to retrieve and localise information. They also allow other cues such as spatialised audio, virtual annotations and stereopsis to aid performance. 
In a wearable setting, spatial information display can be used to overcome the resolution and field of view limitations of the HMD and provide information overlay on the surrounding environment. This is important because the information presented on a wearable is often intimately linked to the users real world location and task. Despite these advantages, most wearables only use headstabilised information display. A notable exception to this is the work of Feiner et. al. [3] who have developed a wearable campus navigation aid that displays worldstabilised virtual labels on surrounding buildings. Although not in a wearable environment, Feiner et. al. [12] have also demonstrated 2D head, bodyand worldstabilised windows in an augmented reality environment. This extended their previous work which the combined a headstabilised virtual display with a laptop screen to overcome the size limitations of the screen [13]. 
To date there have been no usability studies showing the usefulness of spatialised information display on a 
wearable computer. In this paper we provide an empirical comparison between information display types and compare user task performance on the same task with different display styles. We focus on comparing bodystabilised to headstabilised information presentation and also examine how audiovisual spatial cues can be added to bodystabilised spaces to further improve performance. 
A WEARABLE INFORMATION SPACE In our work we have chosen to begin with the simplest form of bodystabilised display; one which uses one degree of orientational freedom to give the user the impression they are surrounded by a virtual cylinder of information. Figure 1.0 contrasts this with the traditional head stabilised wearable information presentation. 
 Users cannot become easily disoriented No additional input devices are needed It is natural to use since most head and body 
A head mounted display allows only the portion of the information space in its field of view to be seen. Thus, there are two ways the data can be viewed in a cylindrical bodystabilised space; by rotating the information space about the users head, or tracking the users head as they look about the space. The first requires no additional hardware and can be done by mapping mouse, switch or voice input to direction and angle of rotation, while the second requires only a simple one degree of freedom tracker. The minimal hardware requirements make cylindrical spatial information displays particularly attractive. In this paper we compare both interactions methods to each other and to information presented in a head stabilised manner. The cylindrical display space also allows us to use audio and visual spatial cues to aid performance which we describe in the later half of the paper. 
IMPLEMENTATION This research was conducted on a custom built 586 wearable PC with 20mb of RAM running Windows 95. A hand held Logitech wireless radio trackball
with three buttons was used as the primary input device. The display was a pair of Virtual iO iglasses! converted into a monoscopic display by the removal of the left eyepiece. The Virtual iO head mounted display can either be used in seethrough or occluded mode, has a resolution of 262 by 200 pixels and a 30 degree field of view. The iglasses! also have a sourceless inertial and gyroscopic three degree of freedom orientation tracker. Figure 2.0 shows a user wearing the display and wearable computer. 
For the purpose of our user studies, a simulated bodystabilised information space was created by texture mapping images to polygons placed in a cylinder around the users viewpoint. Headstabilised information was shown as a stack of texture maps attached to the users view point. The Direct3D graphics library was used and the interface was deliberately kept simple because the wearable has no graphics acceleration hardware. With eight sample images the simulation ran at over 15 frames a second. 
SPATIAL DISPLAY EXPERIMENTS In the following sections we describe two experiments with our wearable interface. The first examines display performance effects and the second the benefit of spatial cues. 
Expt 1: Display Performance Effects In the first experiment we compare how easily users can find information from eight pages of data displayed in the following conditions: 
A) Multiple headstabilised pages: All the pages are stacked on top of one another so that only the top most page is visible. When the user holds the right trackball button down and rolls the trackball in the positive or negative Y direction they scroll forwards and backwards at a constant rate through the stack of pages. The pages are attached to the users viewpoint so changing head orientation has no effect on page shown. 
B) Cylindrical with trackball control: Pages are spaced equally about the surface of a bodystabilised 
cylinder with the user at the centre. Holding the right trackball button down and rolling the trackball in the positive or negative X direction rotates the cylinder clockwise or counter clockwise at a constant rate about the users head. The head tracker is not used so head rotation has no effect on viewpoint. 
C) Cylindrical with head tracking: Pages are displayed on the surface of a cylindrical space as above. When the right trackball button is held down, the yaw angle of the user head motion is measured by the head tracker and used to set the camera viewpoint rotation about the vertical axis. Head motions in other directions (pitch and roll) have no effect on the viewpoint. 
Condition A simulates how data is displayed in most current wearable applications. In all conditions the pages are exactly the same size. A snapto function was used in the cylindrical conditions (B and C) so that when the user released the right button the view would snap to the page taking up most of the visual field. This was to ensure that when the user stopped manipulating the cylindrical space they could only see one page, just as in the headstabilised condition. 
Experimental Task Since many wearable applications involve data display and retrieval, the subjects task was to find which page contained a certain target icon. Eight pages of unique graphical icons were used with five icons shown on each page. A headstabilised target icon was shown in the upper right hand corner of the users display and was visible at all times. Figure 4.0 shows a sample page with target icon. 
In all conditions the trackball buttons were used to start and stop user interaction. When the user released the right trackball button and clicked the middle button the target icon was compared to those on the currently visible page. If there was a match a new target icon was automatically displayed, otherwise the current icon remained. A set of eight target icons
were used for each condition and time measurements were taken between button presses to measure search time. 
Twelve subjects took part in the experiment, seven males and five females aged between 19 and 35. Some of the subjects had experience of virtual environments, but none had used a wearable computer before. Subjects were given a standardised test of spatial ability [14] and all had normal natural or corrected eyesight and normal hearing. Subjects were also all righteye dominant, the eye covered by the monoscopic display. To begin the experiment they were given several minutes training with each condition until they felt comfortable with the interface. While training, a calibration routine measured the amount of time it took each subject to view all the pages under each condition. A Latin squares design was then used; all subjects experienced all three conditions, but in a different order to minimise order effects. Three sets of eight target icons and corresponding pages were created and each subject used the same set of icons. Although the image sets were different, the target icons occurred in the same order in all image sets. 
For each display condition subjects were given a total of three minutes to complete two tasks. First they were to find each of the eight targets as quickly as possible and time to complete each search was measured. After completing the search they were to use the remainder of the time to remember the order of the pages in the information space. Following each condition subjects were tested on the workload of the task using a the NASA standardised workload assessment battery which asked questions about mental, physical, temporal and emotional effort during the task [15]. They were also tested on their recall of the information space by giving them a set of paper images of the pages they had just seen and asking them to place the pages in either a stack or cylinder corresponding to the space they had just experienced. After completing all three trials users were given a post experiment and asked to rate each condition according to ease of use and understanding of where the information was. The complete post experiment questionnaire is shown in appendix A. 
Results Performance The three display conditions had different amounts of inherent system delays, for example condition C required polling the head tracker. A simple normalisation technique was used to produce performance values that could be compared across conditions. The average time it took each subject to view the entire information space under each display condition was measured during the training and preexperiment calibration period. After the experiment, 
the subjects performance time for each condition was then divided by this normalising factor. In calculating the average search time, the first result from each trial was also discarded to ensure that subjects were always starting their search from the same point; the location of the first target object. 
Users performed significantly faster in the two bodystabilised display conditions (B and C). Table 2.0 shows the resulting original average search times, calibration values and normalised values for each condition. Although subjects performed quicker in the head stabilised condition (condition A), there were less inherent delays in this condition, shown by the calibration times. Thus the normalised values give a better indication of relative performance. A one factor ANOVA on the normalised result found these to be significantly different across conditions (F = 4.88, df = (2,24), p=0.016, Fcrit = 3.40). If the system delays in each condition had been the same then subjects would have performed one and a half times faster with bodystabilised information spaces that with a traditional headstabilised interface. There was no difference in performance between bodystabilised conditions and no significant correlation between spatial ability and search times or normalised values. 
Display Condition Results A B C Avge. Search Time 6.07 8.33 8.06 Avge. Calibration Time 7.18 14.31 13.4 Normalised Result 0.91 0.59 0.68 
Information Recall Subjects found it easier to recall the information space in the headtracked condition than in the other conditions. Only two out of the twelve page layouts for this condition were incorrect, as opposed to five each for the nonhead tracked spatial display and the headstabilised display. Subjects commented that it was easy to remember the page ordering in the head tracked condition because the pages always stayed in a fixed position with respect to their body. In some cases they used real world objects which could be seen through the display to help them remember the location of the virtual pages. 
Workload Subjects felt the head tracked condition involved significantly more physical work than the other display conditions. In response to the question, How much physical activity was involved?, on a scale of 1 to 9 (1=low, 9=high) subjects gave the average scores shown in table 5.0. As expected subjects found the head tracked condition more physically demanding; a one factor ANOVA finds a highly significant difference between conditions, [F = 8.70,
df = (2,33), p &lt; 0.0001, Fcrit = 3.28]. There was no significant difference across any of the other workload survey questions. 
Subjective Impressions Users felt the spatialised conditions more enjoyable, intuitive and easier to find the target objects with. Table 6.0 summarises the average values given for the first four post experiment survey questions. In these questions subjects were asked to score conditions on how easy it was to find the target, to remember where all the information was, how enjoyable it was, and how intuitive the interface was. They were asked to score the answers on Likert scales with anchors of 1 at the negative end and 7 at the positive end. The complete survey is shown in appendix A. There was a significant difference across conditions for responses to questions on how easy is was to find the target, how enjoyable it was, and how intuitive it was. Comparing between body stabilised conditions, a onetailed ttest finds that subjects felt it easier to find target with the trackball control than head tracking (t = 1.83, df = 16, p=0.043, tcrit = 1.75). There were no other significant differences between spatialised conditions. 
Conditions ANOVA Questions A B C P value Find Target* 3.6 5 4.3 p &lt; 0.01 Remember 3.8 4.7 4.6 p = 0.32 Enjoyable* 3.3 4.8 4.4 p&lt;0.05 Intuitive* 4.3 4.9 5.5 p&lt;0.025 Table 6.0 Responses to Survey Questions 14. For the ANOVA, in all cases the df = (2,33) 
Subjects were also asked to rank each condition in order for the same questions, where the best condition was ranked first and the worst last. Subjects rankings were significantly different across conditions for how easy it was to find the target, whether they understood where all the information was, and how intuitive the interface was. In all cases subjects ranked the spatialised conditions (B and C) better than the headstabilised condition (A). Table 7.0 summaries the average rankings, the associated KruskalWallis scores (K values) and significance levels. 
Easiest* 2.75 1.5 1.75 10.5 p&lt;0.01 Liked Best 2.50 1.75 1.75 4.5 p&lt;0.20 Understanding* 2.97 1.75 1.33 16.2 p&lt;0.001 Intuitive* 2.67 2.08 1.25 12.2 p&lt;0.005 
As with the first set of questions, subjects found the headtracked condition most intuitive. They also felt the bodystabilized conditions gave a better understanding of where all the information was in the display space. 
Discussion In this first experiment we explored user performance on a search task within headstabilised and bodystabilised information displays. Users performed better in the bodystabilised conditions (taking system delays into account), and also perceived that it was easier to find the target information in such conditions. This performance improvement happened because users had a better understanding of where pages were and also could see upcoming pages as they rotated their viewpoint. 
Although there was no performance difference between bodystabilised conditions, many users commented that they found the headtracked condition more intuitive and natural to use. Head tracking was particularly valuable for recall because users could remember where the information was located relative to their bodies. Several users commented on how the head tracking allowed them to associate pages with real world objects. However users found it more physically demanding and some commented on the social acceptability of using head motions in public spaces. 
Expt 2: Spatial Display Enhancements An advantage of using spatialised displays is that additional audio and visual spatial cues can be presented to aid performance [16], [17], [18]. In this second experiment we examined how spatial cues could affect performance in a headtracked bodystabilised display. In addition to the head tracker bodystabilised condition (condition A) described in experiment 1, three other conditions were tested: 
B) Spatialised audio: A head tracked cylindrical bodystabilised display with a three dimensional spatialised audio cue played at the location of the target page. The audio cue consisted of a sample of white noise. The equal frequency distribution of white noise makes it easy to localise [19]. 
C) Visual cues: A head tracked cylindrical bodystabilised display with headstabilised arrows overlaid on the users field of view to show them which way they should turn their heads. When the target is closer in the clockwise direction, the right arrow is shown, and the left when it is closer in the counter clockwise direction. A square between the arrows changes colour when the user is looking directly at the target. Figure 5.0 shows the visual cues. 
D) Visual cues and spatialised audio: A head tracked cylindrical bodystabilised display with the addition
Head tracking was used in all four conditions, but the same spatial cues could have been applied to a nonhead tracked bodystabilised display such as condition B in the last experiment. 
Real time audio spatialisation was performed entirely on the wearables CPU, causing a significant drop in graphics performance. To remove frame rate effects, spatialised audio was played in all conditions, but the head mount speakers were disconnected for the nonaudio conditions. This ensured a constant frame rate across all conditions. 
The same set of subjects used in the first experiment was used in the second and the task was the same, although new sets of target icons and pages were used. Once again, after the entire experiment they answered the same survey questions listed in appendix A, modified to have four conditions for each question. Subjects were not asked to recreate the information space as in the first experiment. 
Results Performance Adding spatial cues significantly aided performance. Figure 6.0 shows the average search times are 35 percent faster with each of the audio and visual cue conditions than with no additional cues. A onefactor ANOVA finds a significant difference between conditions (F = 8.05, df = (3,44), p &lt; 0.0001, Fcrit = 2.81), but there was no difference in performance between spatial cues (conditions B, C and D), (F = 0.03, df = (2,44), p =0.96, Fcrit = 3.28). Subjects took the same amount of time regardless of the spatial cue used. 
There was a significant correlation between spatial ability and performance when using audioonly cues (R =0.61, df = 12, p&lt;0.05). Subjects with higher spatial ability completed the task in less time. However there was no correlation between spatial ability and performance on any of the other conditions. This may be because a spatial audio cue requires the user to form an accurate mental model of the information space relative to the audio source. A visual annotation provides a more immediate cue which requires fewer mental spatial manipulations. 
Subjective Impressions Users felt that spatial cues made significantly easier to find the target, and made the interface more intuitive to use. Table 8.0 summarises the average scores from the first four questions of the post experiment survey and the corresponding one factor ANOVA values. For the question on how easy it was to find the target, there was also a significant difference between spatial cue conditions, (F = 4.13, df = (2,33), p = 0.025, Fcrit = 32.8). Subjects felt it easier to find the target when there were spatial audio cues, even though there was no difference in performance between cue conditions. 
Find Target* 3.3 5.4 4.5 5.8 p&lt;0.001 Remember 4.0 3.3 3.1 3.2 p = 0.22 Enjoyable 3.8 4.8 4.6 4.9 p = 0.16 Intuitive* 4.1 5.3 4.5 5.4 p &lt; 0.05 Table 8.0 Responses to Survey Questions 14. For 
Subjects also ranked the cue conditions in order for the same questions. Rankings were only significantly different across conditions for the question of how easy it was to find the target. In all cases subjects ranked the spatialised audio conditions better than the other conditions, but not significantly so. Table 8.0
Easiest* 3.3 2.1 2.7 1.9 8.9 p&lt;0.05 Liked Best 3.1 2.1 2.8 2.1 4.6 p&lt;0.20 Understanding 2.5 2.2 2.7 2.6 0.8 p&lt;0.9 Intuitive 3.3 1.9 2.7 2.2 7.5 p&lt;0.1 
Discussion As expected spatial cues significantly helped task performance. More interestingly there was no difference in performance despite the dissimilar nature of the cues. The audio cues gave absolute target location information, telling the user which direction they needed to rotate their head and by how much. In contrast, the visual annotations only gave relative information about target location, showing the user which way they needed to rotate and when they had arrived at the target page. Audio cues also rely on a different sense than the visual cues, and some users commented on the extra visual overloading that the visual cues caused. This may explain why users felt they performed better in the conditions using audio cues (B and D). Several users also mentioned that they found it difficult when both visual and auditory cues were used together and often concentrated on only one of the cues. 
However, the addition of spatial cues didnt increase the understanding that users had of the information space. Users commented on how they attended to the spatial cues rather than the pages when searching for targets, affecting knowledge of the space. 
CONCLUSIONS In this paper we addressed the problem of wearable information displays and have shown how even simple spatialised displays provide benefits over traditional headstabilised displays. Users found the bodystabilised displays easier to use, more enjoyable and more intuitive, and were able to perform significantly better on a search task. The fact that there was no difference in results between the headtracked and nonhead tracked methods of viewing the display space imply that these benefits come from the spatialisation of the information itself and can be achieved with no additional head tracking hardware. However user preferences and information recall results suggest that there may be types of spatial interfaces and tasks where head tracking is desirable. This warrants further investigation. 
Adding spatial cues to the bodystabilised display dramatically improved performance. Audio or visual spatial cues both give the same performance benefit, 
although spatialised audio caused a significant graphics performance decrease due to increased CPU load, suggesting that visual cues may be more practical for current wearable applications. 
These results are only the first in a series of explorations on wearable information displays. In the future we plan to implement some real applications in a bodystabilised space and look at long term use. We will also explore spaces with additional degrees of freedom and different forms of visual and audio cues, including adding absolute information to visual cues and varying sound sources for audio cues. Finally, we will investigate how visual and audio cues can be combined in a more intuitive manner, such as using audio for coarse peripheral navigation and visual cues for selection among objects in the field of view. 
REFERENCES [1] Mann, S. Smart Clothing: The Wearable Computer and WearCam. Personal Technologies, Vol. 1, No. 1, March 1997, SpringerVerlag. [2] Espisito, C. Wearable Computers: FieldTest Results and System Design Guidelines. In Proceedings Interact 97. [3] Feiner, S., MacIntyre, B. Hllerer, T. A Touring Machine: Prototyping 3D Mobile Augmented Reality Systems for Exploring the Urban Environment. To appear in Proceedings of the International Symposium on Wearable Computers, Cambridge, MA, October 1314, 1997. [4] Bass, L., Kasabach, C., Martin, R., Siewiorek, D., Smailagic, A., Stivoric, J. The Design of a Wearable Computer. In Proceedings of CHI 97, Atlanta, Georgia. March 1997, pp.139146. New York: ACM. [5] Thad Starner, Personal Communication 1997. [6] Matias, E., MacKenzie, S., Buxton, W. A Wearable Computer for Use in Microgravity Space and Other NonDesktop Environments. Companion of the CHI '96 Conference on Human Factors in Computing Systems, pp. 6970. New York: ACM. [7] Azuma, R., Bishop, G. Improving Static and Dynamic Registration in an Optical SeeThrough HMD. Proceedings of SIGGRAPH '94 (Orlando, FL, 2429 July 1994), Computer Graphics, Annual Conference Series, 1994, pp. 197204 [8] Reichlen, B. SparcChair: One Hundred Million Pixel Display. In Proc. IEEE VRAIS 93. Seattle WA, September 1822, 1993, pp. 300307. [9] Bowskill, J., Morphett, J., Downie, J. A Taxonomy for Enhanced Reality Systems. Submitted to the International Symposium on Wearable Computing 1997. [10] Rekimoto, J., Nagao, K. The World through the Computer: Computer Augmented Interaction with Real World Environments. In Proceedings of User Interface Software and Technology 95. (UIST '95), November 1995, New York: ACM.
[11] Bajura, M., Fuchs, H., Ohbuchi, R. Merging Virtual Reality with the Real World: Seeing Ultrasound Imagery within the Patient. In Proceedings of SIGGRAPH '92. Computer Graphics 26, no. 2 (July 1992), pp. 203210. [12] Feiner, S., MacIntyre, B., Haupt, M., Solomon, E. Windows on the World, 2D Windows for 3D Augmented Reality. Proc. UIST 93 (ACM Symposium on User Interface Software and Technology), Atlanta GA, November 35, 1993, pp. 145155. [13] Feiner, S., Shamash, A. Hybird User Interfaces: Breeding Virtual Bigger Interfaces for Physically Smaller Computers. In Proc. UIST 91 (ACM Symposium on User Interface Software and Technology), Hilton Head SC, November 1113, 1991, pp. 917. [14] Cube Comparisons Test, In Standard Ability Tests, Educational Testing Service, Princeton, New Jersey, 1976. [15] Workload Assessment Battery, NASA Johnson Space Centre, Houston, 1989. [16] Cohen, M. Integrating Graphical and Audio Windows. Presence: Teleoperators and Virtual Environments, Vol. 1, pp468481. [17] Fadden, D., Braune, R., Wiedemann, J. Spatial Displays as a Means to Increase Pilot Situational Awareness. In Pictorial Communication in Virtual and Real Environments, edited by S. Ellis, Taylor and Francis, London, 1991, pp.172181. [18] Burraston, D., Hollier, M., Hawksford, M. Limitations of Dynamically Controlling the Listening Position in a 3D Ambisonic Environment. In Proceedings of the 102nd Convention of the Audio Engineering Society, March 22nd25th, Munich, Germany, 1997. [19] AltonEverest, F. The Master handbook of Acoustics, 3rd Edition, McGrawHill, New York, 1994. 
APPENDIX A: POST EXPERIMENT SURVEY After the two experiments subject were given the following post experiment survey. 
For experiment two the conditions were modified to: Ahead tracked information display Bhead tracked information display with audio cues Chead tracked information display with visual cues Dhead tracked information display with audio and visual cues. 
Ahead stabilised information display Bcylindrical information display with mouse input Ccylindrical information display with head tracking 
1) How easy was it to find the target? 1 2 3 4 5 6 7 1=not very easy 7=very easy 
For the head stabilised condition (A): For the cylindrical condition with mouse input (B): For the head tracked condition (C): 
For the head stabilised condition (A): For the cylindrical condition with mouse input (B): For the head tracked condition (C): 
3) How enjoyable was this condition? 1 2 3 4 5 6 7 1=not very enjoyable 7=very enjoyable 
For the head stabilised condition (A): For the cylindrical condition with mouse input (B): For the head tracked condition (C): 
4) How intuitive was the interface to use? 1 2 3 4 5 6 7 1=not very intuitive 7=very intuitive 
For the head stabilised condition (A): For the cylindrical condition with mouse input (B): For the head tracked condition (C): 
PART B For the following questions you will be asked to rank all the conditions in order on a scale of one to three and give a brief explanation for the ranking. The three conditions were: 
Ahead stabilised information display Bcylindrical information display with mouse input Ccylindrical information display with head tracking 
3) Which condition did you feel like you had the most understanding of where all the information was (1 = most understanding, 3 = least understanding) 
