M. Billinghursta, D. Belchera, A. Guptaa, K. Kiyokawaa,b a Human Interface Technology Laboratory, University of Washington, 
Box 352142, Seattle, WA 98195, USA b Emergency Communication Section, Communications Research Laboratory, 
We present an analysis of communication behavior in facetoface collaboration using a multiuser Augmented Reality (AR) interface. We conducted two experiments. In the first, collaboration with AR technology was compared to more traditional unmediated and screenbased collaboration. In the second we compare collaboration with three different AR displays. Several measures are used to analyze communication behavior, and we found that users exhibited many of the same behaviors in a collaborative AR interface as in facetoface unmediated collaboration. User communication behavior also changed with the type of AR display used. We describe implications of these results for the design of collaborative AR interfaces and directions for future research. 
For several decades researchers have explored how to use computers to support human collaboration. The field of Computer Supported Collaborative Work (CSCW) contains numerous examples of interfaces that facilitate mediated collaboration. However much of this work is focused on remote collaboration. 
In contrast we are interested in developing interfaces for facetoface collaboration, and in particular collaboration on tasks that involve viewing and manipulating spatial data. Many different interfaces have been developed for supporting colocated collaboration. Some have been based around computer conference rooms. For example, the CoLab room at Xerox [Stefik 87] used a network of workstations and a shared electronic whiteboard to support small group meetings. More recently, the iLAND interface integrates several components into a combination of real and digital work environments for creative teams [Strietz 99]. This is based on a concept called Roomware where computeraugmented objects in a room can be dynamically reconfigured to support facetoface collaboration. The largest of these components is the DynaWall, an interactive electronic wall that serves as a large common display. 
While these interfaces are good for supporting documentcentered tasks they have some limitations for collaborative viewing and interaction with spatial data. For example, it is often difficult to manipulate 3D data on a 2D screen or with traditional input devices [LiShu 94]. Screenbased interfaces also create a separation between the real and digital domains. Real objects and interactions within the real world play an important role in facetoface collaboration [Minneman 96]. However, apart from Tangible User Interfaces such as the MetaDesk [Ullmer 97] or Fjelds 
Bricks [Fjeld 99], most colocated CSCW systems do not support objectbased collaboration or include elements of the real world. 
A subtler problem is that many interfaces for facetoface collaboration artificially separate the communication and task space. When people sit at a table, the space between them is used for sharing communication cues such as gaze, gesture and nonverbal behaviors. If the people are communicating about objects placed on the table then the taskspace is a subset of the communication space (figure 1). However when users are collaborating in front of a screen their attention is focused on the screen (figure 2). So collaborators may exhibit different communication behaviors when using a screenbased interface than when seeing each other across a table. 
Alternative techniques that attempt to overcome these limitations include using large screens to project a threedimensional virtual image into space. The CAVE [CruzNeira 92] and the Responsive Workbench [Kruger 95] allow a number of users to view stereoscopic 3D images by wearing LCDshutter glasses. Unfortunately in this environment it is difficult to support more than two independent viewpoints. Mechanical devices can also be used to create true volumetric displays. These include scanning lasers onto a rotating helix to create a threedimensional volumetric display [Soltan 95] or projecting images onto a rotating plate [Actuality 2001]. Although these systems support multiple viewpoints, they do not allow direct interaction with the virtual content because of the rotating display surface. 
In our work we use Augmented Reality (AR) technology to overlaying 3D virtual imagery directly onto the real world. In this way the display space and communication space overlap. Other researchers have found that collaborative AR interfaces can enhance colocated collaboration and facilitate very natural facetoface communication [Schmalsteig 1996], [Ohshima 1998]. This is because users can view and manipulate virtual models while seeing each other in the real world, allowing natural nonverbal communication cues to regulate the collaboration [Kiyokawa 99]. In AR interfaces real objects can be used to interact with the virtual content, further increasing the intuitiveness of the interface. 
Although several collaborative AR interfaces have been developed, there have been few rigorous user studies conducted. In comparison, other research in teleconferencing has evaluated the effect of technology on communication behaviors. For example, Isaacs compares audioonly conferencing to video conferencing and analyzes the differences in speaker behavior [Isaacs 93], 
while Krauss has studied the effect of delay in a conferencing link on communication behavior [Krauss 67]. Similar experiments are needed with collaborative AR interfaces. 
In this paper we describe two communication experiments. In the first, we compare collaboration with AR technology to more traditional unmediated and screenbased collaboration. In the second we compare natural facetoface collaboration to that with three different AR displays. A variety of experimental measures are used, as discussed in the next section. Following this discussion we present results from our user studies and directions for future research. 
In this section we draw on previous user studies to describe a set of metrics and methods for evaluating collaborative AR interfaces. For decades researchers have been conducting studies on the effect of technology on collaboration. Researchers such as Monk et. al. [Monk 96] argue that a multidimensional approach to evaluating videomediated collaboration is needed to capture all the effects of the technology used. So in our work we use a variety of performance, process and subjective measures. Performance measures are those that measure a task outcome, such as the time it took to complete a task. Process measures are conversational elements that occur during the collaboration, such as the number of words spoken. Subjective measures are the participants own subjective impressions of the collaboration. 2.1 Performance Measures Until recently most mediated communication experiments only used performance measures to differentiate between communication conditions. For example, Chapanis compared how quickly subjects could communicate using ten different modalities such as audio only, video, writing and unmediated facetoface communication [Chapanis 75]. Typical performance measures include how fast a task can be completed, and the quality of the outcome of the collaboration. However, task outcome is often a poor measure of the effect on communication of different technology. Indeed, in many telecommunication experiments there were no performance differences between mediated conditions [Williams 77]. This may be because subjects try to protect their primary task of getting the work done [McCarthy 94], which they may do through increased workload, a factor not measured by performanceoriented studies. Hockey argues that this can be observed in measures that look at the process of task performance [Hockey 83]. In our user studies we measure performance time, but only to provide a gross measure of the difference between conditions. 2.2 Process Measures Process measures are objective communication measures that capture the process of collaboration. These are extracted from transcriptions of audio and video recordings and notes made during the collaborative task. With the right process measures considerable differences between technology conditions can be found. For example, Rutter et. al. found that there was significantly more simultaneous speech in a face to face meeting than in a video conferencing setting [Rutter 81]. 
One of the difficulties with collecting process measures is deciding which metrics to use. Nyerges et. al. provide a good introduction to the art of coding groupware interactions and give guidance on good metrics to code for [Nyerges 98]. Measures that have been found to be significantly different across technology conditions include: 
Frequency of conversational turns [DalyJones 98] [OConaill 97] [O'Malley 96] Incidence/duration of overlapping speech [DalyJones 98] [OConaill 97] 
Number of interruptions [Boyle 94] [OConaill 97] Turn Completions [Tang 92] Dialogue structure [Boyle 94] [O'Malley 96] Backchannels [OConaill 97] 
Gesture and nonverbal behaviors can also be analyzed for characteristic features. Bekker et. al. [Bekker 95] identified four classes of gesture exhibited in a facetoface design task; kinetic, spatial, pointing and other. These four categories were based on the more complex coding categories used by McNeill [McNeill 92] and Ekman and Friesen [Ekman 69]. In our work we transcribe videotapes for both lowlevel speech and gesture features. 2.3 Subjective Measures Subjective measures are based entirely on the users perception of their experience. The typical method is to have users fill out a survey questionnaire after each experimental condition [Sellen 92]. DalyJones provides a set of questions that have been found to be sensitive to the differences in mediating technology [DalyJones 98]. These questions refer to interpersonal awareness, ease of communication, and the suitability of the communication mode for the task. For example; 
I was very aware of the presence of my conversational partner. I could readily tell when my partner was concentrating on what I was saying. 
In our experiments we used a modified version of the DalyJones survey questions as well as postexperiment interviews to capture the users experience. 
3 EXPT 1: COMMUNICATION IN A COLLABORATIVE AR INTERFACE In the first experiment we compared collaboration with an AR interface to a projection display and to unmediated facetoface interaction. The three different conditions tested were: 
FtF: Unmediated facetoface collaboration AR: Augmented reality facetoface collaboration Proj: Projection screenbased collaboration 
The main differences between these conditions were in the viewpoints of the collaborators and the method for interacting with the shared objects (see table 1). 
By considering the innate characteristics or affordances of each condition we can predict the impact on communication behaviors. In the FtF and AR conditions users can more easily share nonverbal cues, and use gestures to interact with objects. Thus we should see a difference in 
language and behavior between the FtF and AR conditions and the projection screen condition. For example, we should see more deictic language used in the FtF and AR conditions, and more gestures and communication spread across the verbal and nonverbal channels. 
However, viewing the world through an AR display is very different from seeing it naturally. The field of view may be limited, the resolution poor, and the display may not support stereoscopic viewing. These factors combine to give a mediated viewing experience that is very different from natural facetoface collaboration and they may also affect the communication behaviors. 
Pairs of users were engaged in an urban design task. This involved placing nine real or virtual model buildings on a 3x3 block street layout on a table. The virtual models were exact copies of the real buildings. For each condition the buildings needed to be placed to satisfy ten rules, such as: 
The CHURCH is next to the THEATER The FIRE HOUSE and the BANK are across the street. The CITY HALL is in the middle of town. 
The rules were designed to be complementary and each subject was only given five of them. Subjects were told they could not show their partner the piece of paper that the rules were written on, but could freely use speech and gesture to collaborate. This was to encourage them to communicate together, even if just to read out the rules they had. They were given seven minutes to complete the task. Before beginning the task they were given several minutes to familiarize themselves with the real model buildings so they could easily recall what each building was. 
FtF/ Face to face interaction: The subjects sat on opposite sides of a table (See figures 3,4) and attempted to layout a set of nine real model buildings subject to the constraints mentioned above. 
AR/ Augmented Reality Interaction: The subjects were given a set of nine cards with Japanese Kanji characters marked on them (figure 5). They also wore the DaeYang CyVisor headmounted display with a small video camera attached (figure 6). These displays are full color, bioccular rather than stereoscopic, have SVGA (800x600 pixel) resolution and a 30degree field of view. When they looked at the marked card a computer vision based tracking method was used to track 
the cards and overlay virtual buildings on them (figure 7) [ARToolKit 2001]. Before beginning this condition subjects were given several minutes to practice with the AR content. 
Proj/ Projection Screen Interface: Subjects sat sidebyside at one end of a table facing a large projection screen (figure 8). On the table were two cards, each with a Polhemus Fastrak magnetic tracker and a button mounted on it. These trackers were used to find the position and orientation of the card relative to a fixed magnetic source. Shown on the screen was a graphical interface that allowed subjects to place virtual models of buildings on a flat ground plane (figure 9). As they move the Fastrak sensors virtual markers on the screen move in the same way. Buildings could be selected and placed using the button on the tracker. This interface was developed using Sense8s WorldUp software [Sense8 2001]. Before beginning this condition subjects were given training on how to use the interface and practiced placing buildings until comfortable with the software. 
A projection screen interface using two six degree of freedom input devices was chosen because it supports simultaneous user input, enabling users to move and place the buildings in parallel. Clearly the projection condition creates a separation between the task space (the projection screen), and the communication space (the users at the table), when compared to the other conditions. 
A withinsubjects design was used so that each pair of subjects experienced all three interface conditions. The conditions were presented in a counterbalanced manner to reduce order effects. 
3.2 Results The subjects in this experiment were 14 pairs of college age adults, 6 pairs of woman and 8 pairs of men. Ages ranged from 21 year to 38 years old. Several of these pairs were later dropped from the study due to incomplete results, so the final results were drawn from 12 pairs of subjects. 
Performance The subjects were given seven minutes to complete the logic puzzle task. They were also told that it was important to satisfy all the ten rules. There was a significant difference between puzzle solution times across conditions. Table 2 shows the average solution times. Using a nonparametric KruskalWallis test on the log of the solution times, we found H = 13.0, d.f. = 2, P &lt; 0.002. In order to compare between conditions we used a MannWhitney test. Doing so we find that there was no significant difference in performance between the FacetoFace and Projection condition (Z =1.44, N = 26, P = .151), however there was a significant difference between the Projection and AR conditions (Z =2.21, N = 26, P &lt; 0.05) and between the FacetoFace and AR conditions (Z =3.51, N = 26, P &lt; 0.01). Thus users solved the puzzle significantly slower in the AR condition. 
 Face to Face Projection Augmented Reality Average Solution Time (Sec) 163.8 195.8 270.7 Std. Dev. (Sec) 52.3 60.9 61.8 
Communication Process Measures Complete transcriptions were made of the speech communication between pairs as they completed each of the conditions. These were then coded for low and highlevel speech behaviors. The video of each pair of collaborators was also watched and coded for nonverbal and gestural behaviors. The communication measures collected included: 
Turntaking From the video transcriptions the number of speaker turns and the average number of words per turn were measured. Table 3 shows these results. Uisng a onefactor ANOVA we found that there was no difference in the average number of words per turn across conditions (F(2,30) = 1.37, P = 0.27). There was also no difference in the number of turns per second (F(2,30) = 0.23, P = 0.79). 
 Face to Face Projection Augmented Reality Average words/turn 9.51 8.99 8.21 Average turns/second 0.30 0.311 0.32 
 Deictic Phrases The number of deictic phrases used was counted. These are phrases with the words this, that or there that cannot be fully understood by considering the speech alone. Table 4 shows the average percentage of deictic phrases out of the total number of phrases spoken in each condition. Using a nonparametric KruskalWallis test, we found H = 7.58, d.f. = 2, P &lt; 0.05. So there is a significant difference in percentage of deictic phrases used across conditions. Using a MannWhitney test we find no difference in deictic percentages between the FacetoFace and Projection conditions (P = 0.365) or the FacetoFace and AR conditions (P = 0.116), but there is a significant difference between the Projection and AR conditions (Z =2.79, N = 22, P &lt; 0.01). So users in the AR condition used more proportionally more deictic phrases than in the Projection condition. 
 Face to Face Projection Augmented Reality Avg. deictic percent. 17.4% 13.4% 22.6% Std. Dev. 7.4% 5.0% 7.9% 
The subject videotapes were watched and the number of turns counted where simultaneous speech occurred. Simultaneous speech is defined as when both speakers are speaking at the same time. This includes types of speech such as interruptions that result in a change of speaker turn and completions where one speaker completes the others sentence. Table 5 shows the amount of simultaneous speech as a percentage of total speaker turns across all three conditions. There was no difference in the percentage of simultaneous speech across conditions. 
From the videotape transcripts we counted the number of user turns that contained one or more questions and calculated a percentage of turns with questions in them relative to the total number of all speaking turns for each user. As the results in table 6 show, in the AR condition subjects used questions more frequently that in the other conditions. These results are significantly different. Using a KruskalWallis test, we found H = 6.83, d.f. = 2, P &lt; 0.05. When we performed a pair wise comparison with a MannWhitney test, we found a significant difference between the AR and FacetoFace conditions (Z =2.37, N = 22, P &lt; 0.05), and between the AR and Projection conditions (Z =2.07, N = 22, P &lt; 0.05), but not between the FacetoFace and Projection conditions (Z =0.49, N = 22, P = 0.65). Thus users ask proportionally significantly more questions in the AR case than the other two conditions. 
 Face to Face Projection Augmented Reality Percentage of Questions 14.9% 16.4% 22.7% Std. Dev. 6.7% 5.5% 7.0% 
However, many of the questions asked in the AR condition related to building identification. For example Where is the bank?, What is that building you are holding?, and Thats the Firehouse ?. The prevalence of these questions illustrates the reduction in perceptual cues caused by the AR HMD. When we remove these questions from those used to calculate the question percentages we find little difference across conditions. These are shown in table 7. There was no significant difference across conditions (H= 1.46, d.f. = 2, P = 0.93). 
 Face to Face Projection Augmented Reality Percentage of Questions 13.6% 14.1% 14.1% Std. Dev. 6.4% 5.3% 4.3% 
Gestural Communication Patterns We also counted the number of gestures made in each condition and categorized them into one of four types: 
POINT: A pointing gesture PICK: A gesture that involved picking or moving an object COLLAB: A collaborative gesture such as passing an object between people OTHER: Any other gesture 
Figure 10 shows the percentage breakdown for each type of gesture made out of all the gestures for each condition. As can be seen, in the Projection condition the majority of gestures made (66%) are Pick gestures, while Point gestures make up less than (30%). In contrast the percentage of Pick and Point gestures in the FacetoFace and AR conditions are very similar. There was a significant difference between the percentage of Pick gestures made, (H= 7.67, d.f. = 2, P &lt; 0.05), but not between the percentage of Point gestures made, (H= 4.42, d.f. = 2, P = 0.11). In the case of Pick gestures, there was no difference between AR and FacetoFace conditions (Z =0.68, N = 22, P = 0.49), but there was a significant difference between the FacetoFace and Projection conditions (Z =2.53, N = 22, P &lt; 0.05), and between the AR and Projection conditions (Z =2.15, N = 22, P &lt; 0.05). There is a significantly higher percentage of picking gestures in the Projection condition than in either of the other two conditions. 
Subjective Measures After each condition users filled out a survey on how they felt about the interface and how well they could collaborate with their partner. All of the questions were answered on a scale of one to seven, with one being Not Very Easily and seven being Very Easily. For example: 
When asked the question How easily could you work with the other person?, people felt that it was easiest to work together in the FacetoFace condition, and that there was a significant difference in working together between conditions (figure 11), a onefactor ANOVA giving F(2,69) = 5.96, P &lt; 0.005. Users felt that it was as difficult to work together in the AR condition as it was in the Projection condition. Similarly, when asked How easily could you understand your partner?, users felt that it was significantly easier to understand each other in the face to face condition, while the Projection and AR conditions were as difficult to understand each other (figure 11), (F(2,69) = 6.31, P &lt; 0.005). 
When asked how easily they could tell where their partner was looking and pointing, users felt that perceiving these nonverbal cues was far easier in the facetoface condition and there was little or no advantage in using the AR display compared to the Projection display condition (figure 12). A one factor ANOVA finds a highly significant difference in the average answers to both these questions. For How easily could you tell where the other person was looking, F(2,69)= 25.4, P&lt;1x108, while for How easily could you tell where the other person was pointing, F(2,69)= 14.2, P&lt;1x104. 
Users were also asked how easy they felt it was to pick and move the buildings. As figure 13 shows, they felt that it was nearly as easy to pick and move the AR objects as it was the real models, and significantly easier than with the projection screen interface. In response to the question, How easily could you pick up the buildings, a one factor ANOVA gives (F(2,39)= 37.8, P&lt;1x108). While for the question, How easily could you move the buildings, we find (F(2,39) = 28.4, P&lt;1x107). 
Subject Comments After subjects completed the experimental task they were also interviewed for 1520 minutes to capture their impressions. In general, users felt that while the AR condition was enjoyable there were some usability issues that should be addressed. In ten out of fourteen interviews users mentioned perceptual problems such as limited field of view, low resolution and blurry imagery. Several users felt that the limited field of view forced them to consciously switch modes in the AR task, for example between reading the rules or placing buildings, or between collaborating with their partner and placing houses. An interesting comment was from two users who felt that the AR condition created a form of tunnel vision. 
In addition to the perceptual issues, many users commented on the limitations of the visionbased tracking. In half of the interviews the problem of the virtual buildings flicking in and out of view was mentioned. Several people mentioned that this made the buildings feel less real, or that they 
were reluctant to point at virtual objects because this would make the buildings disappear. These factors taken together meant that users felt they had to concentrate on seeing in the AR case. 
Despite these perception problems users felt that interaction with the virtual objects was easier in the AR condition that in the Projection condition. This was partly due to the tangible interaction metaphor used that made the virtual buildings seem more real. For example, one user said that: 
ARs biggest limit was lack of peripheral vision. The interaction physically (trading buildings back and forth) as well as spatial movement was natural, it was just a little difficult to see. By contrast in the Projection condition you could see everything beautifully but interaction was tough because the interface didnt feel instinctive. 
Another user said AR is more real since you are actually holding something just like the real thing. In contrast, in eight out of the fourteen interviews subjects mentioned the difficulty of interacting with objects in the Projection condition. For some people this was because of the mouse like interface used, while in six interviews people mentioned how their hands kept on bumping into their collaborators hands. Some subjects said that this was due to the difficulty of seeing their real hands while focusing on the projection screen in front of them. 
Users also commented on the difference in viewpoints between the conditions. In seven interviews subjects said that in the face to face condition they were able to make eye contact and see the gaze direction of their partner sitting across the table. However in the AR condition the narrow field of view and bulky display made it impossible to see gaze direction. In both the AR and FacetoFace conditions the subjects said that most of their attention was focused on the task space between them, and they felt that they could see the gestures that were being made. In two of the interviews subjects talked about how having different viewpoints made them feel that they were working independently, or as one person put it working solo together. This was not the case with the projection condition. Some users commented that sharing the same viewpoint in the projection condition helped performance because they felt that they were working toward the same goal, and in four of the interviews subjects said that they did not need to see the gaze of their collaborator because they were looking at the scene from the same viewpoint. Others felt that not facing their partner made the Projection condition the most difficult. For example one subject wrote: 
The AR condition was better than the Projection condition because I could point and see my partner and where she pointed. 
In general these results show that although users did not feel that the AR and FacetoFace conditions were very similar they used the same gesture and speech patterns in these conditions, and very different behavior from the Projection condition. In their deictic speech patterns there was no difference between the FacetoFace and AR conditions, and their picking behavior in the AR condition was more similar to the FacetoFace condition than the Projection condition. 
The tangible interface metaphor seems particularly successful. Even though they are hampered by the narrow field of view and low resolution of the AR display, users felt that they can pick and move objects as easily in the AR condition as in the FacetoFace condition and far more easily than the Projection condition. This allowed them to easily pass objects between each other resulting in the same number per minute of collaborative gestures in the AR and FacetoFace conditions. 
From the interviews after the experiment it seems that the perceived difference between the AR condition and other two conditions can be explained by the reduced perceptual cues supported by current AR technology. For example, even though subjects were sitting across the table from one another, in the AR case they had no peripheral vision, making it more difficult for them to recognize when their partner was looking at them. It is encouraging that even with reduced perceptual cues subjects are exhibiting behaviors in the AR case that are similar to unmediated FacetoFace collaboration and significantly different from traditional interfaces. Higher resolution AR displays with a larger field of view should produce results even more similar to unmediated FacetoFace collaboration. 
There are many possible form factors for Augmented Reality displays. In this experiment we just used one, a head mounted display, that was worn all the time and that reduced perception of the real world and available peripheral cues. Other display types might not have these same characteristics. In order to explore this further we conducted a second experiment in which the influence of form factor on collaboration was explored. We report these results next. 
4 EXPT 2: THE EFFECT OF DISPLAY TECHNOLOGY In the previous experiment we compared unmediated facetoface collaboration to collaboration with a projection screen and with an AR interface. Significant differences were found between the three conditions, in both the way collaborators communicated with each other, and in their actions. 
However, the limited resolution and field of view of an AR HMD reduces the perception of communication cues and may impact facetoface collaboration. For example, the CyVisor display used in experiment only had a 31 degree field of view, reducing the peripheral cues that could be seen. Over half of the subjects commented on the reduced perception due to the display limitations. However, there are many possible designs for AR displays, some head worn, some hand held, some desk mounted. So it is important to understand the impact of AR display affordances on the collaborative process. This was the goal of the followon experiment. 
In this experiment we want to compare facetoface collaboration with three different AR displays: LCD a 5 inch diagonal Liquid Crystal Display (LCD) with a camera on the back (figure 14a). HHD a handheld display (figure 14b). An Olympus FMD150 display modified to be mounted on a handle with a camera. This is held in one hand and can be held against the face or taken away. HMD a head mounted Augmented Reality display (see figure 14c). A Canon GT270 display with a camera mounted on it. This display leaves both hands free when being used. 
The field of view occupied by the LCD display depends on how far away from the users eyes it is held. For example, if the display is held at arms length, then the field of view is about 5 degrees. 
As can be seen the resolution of the displays is similar and two of them have almost the same field of view. However, one difference is in the amount of peripheral awareness they support. The Canon HMD is in front of the eyes at all times and gives no peripheral awareness beyond its 31 degree field of view. The Olympus hand held display can be easily removed from in front of the face, allowing the user to easily see either the real world or the mediated view of the world with virtual graphics overlay. The LCD can be held at arms length, allowing the user to see both the unmediated and mediated views of the world at the same time, thus supporting the greatest peripheral vision. 
Previous research has shown that peripheral awareness is important in supporting colocated collaboration. So we predict that the handheld and LCD display configuration will impact communication less than the head mounted display. However since the handheld display has a greater field of view than the LCD panel, users may perform better using this display. 
In order to explore the effect of display affordances on face to face collaboration we engaged pairs of users in the same urban design task used for the first experiment; arranging models of buildings on a simple street grid plan. They did this for each of four conditions: 
FtF/ FacetoFace interaction: Two subjects sat across a table with a street map marked on it. They were given a set of nine model buildings to layout on the street map and were free to lay the buildings out wherever they like subject to the same rules used in experiment one. 
LCD/ LCD Display: As above, but the subjects each held an LCD, and manipulated a set of 9 marked tracking cards. When they looked at these cards through the LCD panel users saw virtual models of the real models they used in the facetoface condition. They were free to lay the buildings out wherever they like subject to the same rules used in experiment one. 
Each pair of subjects first experienced the facetoface condition, then each of the three AR conditions in a counterbalanced order. As in the first experiment, for each condition each subject was given a set of five rules and the goal was to place nine buildings in a way that satisfied all ten rules. Unlike the first experiment, users were allowed to take as long as they need to complete the building layout task, although each condition was typically completed in five to ten minutes. 
The experimental measures used in this experiments were the same as those used in the first experiment, including: 
Performance Measures: The time to complete the building layout task. Communication Process Measures: Turntaking, Deictic Speech, Questions, Gestures, Simultaneous Speech Subjective Measures: Postcondition surveys, Interviews 
For all of these measures the FtF condition (normal facetoface collaboration) is used as a baseline against which all the other measures are compared. We predict that the results from the LCD and HHD display conditions will be more similar to the FtF Condition than the HMD condition. If this is indeed the case then HMDs may have a negative impact on colocated collaboration compared to other AR displays. 
The subjects in this experiment were 12 pairs of college age adults, 4 pairs of woman and 8 pairs of men, ranging in age from 20 to 37 years old. 
Performance Table 9 shows the average time to solve the design task across conditions. The FacetoFace (FtF) condition produced the quickest average solution time, while the hand held display (HHD) condition was the fastest of the three AR conditions. However, there was no significant difference between the four conditions; a KruskalWallis test gave (H= 6.14, d.f. = 2, P = 0.105). As in the first experiment, users took longer in the HMD condition to complete the task. When we used a MannWhitney test to compare between the FtF and HMD conditions we found a nearly significant result (Z =1.848, N = 12, P = 0.068). 
 FtF LCD HHD HMD Avge. Task Time (Sec.) 329.1 422.8 366.3 463.9 Std. Dev. 170.1 191.6 217.4 195.1 
Communication Process Measures Video was recorded of the participants while they performed the urban design task. However due to the length of the solution times (up to 15 minutes in some cases) we did not make a complete transcription of the communication. Instead we selected 2 minutes of interaction in the middle of each task and transcribed this. It was possible for us to completely code the entire tape for nonverbal behaviors. 
Turntaking From the video transcriptions the number of speaker turns and the average number of words per turn was measured. For each pair these were then normalized relative to the results in the FacetoFace condition. Table 10 shows the average and normalized results. The handheld display produced results closest to the FtF condition, but there was no difference in the normalized average number of words per turn across the three AR conditions (H= 0.796, d.f. = 2, P = 0.672). There was also no difference in the normalized number of turns per second across conditions (H= 0.227, d.f. = 2, P = 0.893). 
Table 10: Average words per turn and turns/second. Deictic Phrases The number of Deictic phrases were also counted. Table 11 shows the average percentage of deictic phrases out of the total number of phrases spoken in each condition. There is no significant difference across conditions; (H= 1.503, d.f. = 2, P = 0.682). 
Questions Asked From the videotape transcripts we counted the number of user turns that contained one or more questions and calculated a percentage of turns with questions in them. As the results in table 12 show, there was no difference in percentage of questions across conditions; (H= 0.467, d.f. = 3, P = 0.92). However, as in the first experiment, many of the questions asked in the AR conditions related to building identification. Table 13 shows the percentage of questions asked identifying buildings. There was a significant difference in these results, (H= 12.5, d.f. = 3, P &lt; 0.01). Using a MannWhitney test to compare between AR conditions we find that there are significantly fewer questions trying to identify buildings in the HHD than the LCD conditions (Z =2.028, N = 13, P &lt; 0.05), and a near significant difference between the HHD and HMD conditions (Z =1.874, N = 13, P = 0.064). There was no difference between the LCD and HMD conditions (Z =0.487, N = 13, P = 0.65). So users asked less building identification questions in the handheld display condition than the other two AR conditions. 
Nonverbal Communication Patterns We also counted the number of gestures made in each condition. As before, these were categorized into one of four types: 
POINT: A pointing gesture PICK: A gesture that involved picking or moving an object COLLAB: A collaborative gesture such as passing an object between people OTHER: Any other gesture 
Figure 15 shows the percentage breakdown for each type of gesture made out of all the gestures for each condition. As can be seen, there was a smaller percentage of Pick gestures made in the Face to Face condition than in the AR conditons. There was a significant difference across conditions in the percentage of Pick gestures; H = 8.47, d.f. = 3, P &lt; 0.05. There was a larger percentage of Point gestures in the Face to Face condition than in the AR conditions. There was also a significant difference in the percentage of Point gestures across conditions; H = 8.91, d.f. = 3, P &lt; 0.05. However there was no difference in pick or point gesture percentages across just the AR conditions; for picking, H = 3.814, d.f. = 2, P = 0.149, while for pointing H = 3.128, d.f. = 2, P = 0.209. 
We also counted the amount of time that the subjects were standing. Although subject initially started the experiment seated they were free to stand and lean over the table during the task. Table 14 shows the average percentage of time that subjects were standing for each of the conditions. As can be seen subjects almost never stood in the facetoface condition, but were on their feet over 50% of the time in the LCD condition. There was a highly significant difference between these results; H= 29.7, d.f. = 3, P &lt; 0.0001. Using a MannWhitney test to compare between AR conditions we found a significant difference between the amount of time spent standing in the LCD and HMD conditions (Z =2.66, N = 24, P &lt; 0.01) but not the HHD and HMD conditions (Z =1.25, N = 24, P = .212) or the LCD and HHD conditions (Z =1.09, N = 24, P = .276). Users stood more in the LCD case to see more of the task space in the small field of view display. 
Subjective Measures After the task for each condition was over subjects filled out the same survey that was used in the first experiment. They were asked to evaluate how easy it was to pick and move buildings by answering the following questions on a scale of one to seven: 
How easily could you pick up the buildings (1 = not very easily, 7 = very easily)? How easily could you move the buildings (1 = not very easily, 7 = very easily)? 
The average responses are shown below in figure 16. As can be seen, users felt it was easiest to pick and move objects in the Facetoface condition. There is a significant difference across conditions for both questions. For the picking results the ANOVA test gives F(3,68) = 6.23, P &lt; 0.01. For the moving results we find F(3,68) = 3.79, P &lt; 0.05. It is interesting that even though the LCD panels require the use of at least one hand, users did not perceive them to be worse than the 
other displays for supporting interaction with the buildings. Using a paired ttest we also find a significant difference between the average facetoface picking value and that for the LCD condition, t(25) = 3.64, P &lt; 0.001. Similarly, a paired ttest also gives a significant difference between the average facetoface moving value and that for the LCD condition, t(25) = 2.35, P &lt; 0.05. 
To evaluate their opinion of the ease of collaboration, users were asked to answer the following questions on a scale of one to seven: 
 How easily could you work with the other person (1 = not very easily, 7 = very easily)? How easily could you understand the other person (1 = not very easily, 7 = very easily)? The average responses are shown below in figure 17. Users felt that it was significantly easier to work together and understand each other in the FacetoFace condition. For the working together results the ANOVA test gives F(3,92) = 8.53, P &lt; 0.0001. For the moving results we find F(3,92) = 5.01, P &lt; 0.01. Across the AR display conditions the handheld display condition was felt to be easiest to work with the other person and to understand them, although not significantly different from the other display conditions. 
Finally, users were asked to evaluate how how easy it was to perceive nonverbal cues by answering the following questions on a scale of one to seven: 
How easily could you tell where the other person was looking (1 = not very easily, 7 = very easily)? How easily could you tell where the other person was pointing (1 = not very easily, 7 = very easily)? 
The average responses are shown below in figure 18. There was a highly significant difference between conditions for looking, F(3,92) = 9.98, P &lt; 0.00001, there was also a significant difference between just the AR display conditions F(2,69) = 3.81, P &lt; 0.05. As expected, the more the display covers the users eyes the more difficult it is for them to tell where their partner is pointing. There was also a highly significant difference between conditions for pointing F(3,92) = 15.71, P&lt; 1x107, and a nearly significant difference between just the AR display conditions F(2,69) = 2.54, P = 0.08. Again, users felt that those display conditions that allowed them to naturally see some of the real world allowed them to most easily see where their partner was pointing. 
These results were confirmed when users were asked to rank the following statements on how much they agreed or disagreed with them (1=Disagree, 7=Agree): 
I could tell when my partner was concentrating I was very aware of the presence of my partner I could tell when my partner was looking at me I could tell when my partner was listening to me 
Figure 19 below shows the average rankings for each condition. As can be seen the FacetoFace condition again ranked highest for each of the questions. Table 14 shows ANOVA results for each of the rankings across all the conditions. There was a significant difference in average response for each of the statements across all conditions. When just the AR display conditions are considered, there was a significant difference in how aware the subjects felt of the presence of their partners and their ability to tell when their partner was looking at them. In response to these statements the average score for the headmounted display was much lower than the other AR display conditions, and on average the HMD condition produces the lowest scores across all statements. Clearly the subjects felt that the display form factor affected their ability to perceive their partner and nonverbal conversational cues. 
Subject Comments Subjects felt that the biggest difference between the AR display conditions was in the ability to see their partner. In the majority of interviews, (ten from twelve), subjects commented on how easy it was to look at their partner in the LCD condition. They did not need to move anything away from their face and could even see the LCD screen at the same time as looking at their partner in the real world. In three cases users specifically commented on how their communication was not blocked by anything covering their face. Conversely in ten of twelve interviews users commented on how the HMD condition made their partner seem distant, or made them feel detached from the real world. As one subject said about the HMD display condition; When I was thinking inside the box he wasnt in the box. In contrast, in only two interviews people said that they felt confined by the handheld display, while in another two cases people commented on the peripheral cues the handheld display provided. These differences between the display conditions were summed up by on subject with the following statement: 
The LCD provided no barriers to understanding. The handheld display sometimes makes it hard to see where he was pointing. The HMD display made him seem distant. 
Of course there was almost an inverse relationship in the ability to see their partner and the ability to see the virtual content. People commented on the narrow field of view of the LCD panel (four interviews) and how the virtual models looked flat (two cases). However for the HMD case, in two interviews people talked about how much they liked the sense of immersion they got and how they could easily see their partner at the same time as the virtual buildings. Neither of these points was mentioned about the handheld display. 
People felt that the physical form factor of the different displays affected their interaction. Although most people felt that using the HMD made it most difficult to collaborate with their partner, in seven interviews subjects commented on how much they enjoyed having both hands free to move the models. In seven of the interviews people also mentioned that the HMD has the advantage that it was always on their face, so in order to see different portions of the task space they just had to turn their head. In contrast, some users with the handheld display felt that is was awkward to keep on remembering to hold the display to your face (five of twelve interviews), and difficult to move the display and your head at the same time so that they could keep viewing the AR scene. Two subjects felt that the handheld display was easiest to use, while in three interviews subjects mentioned how the handheld display felt like a magnifying glass, or a pair of opera glasses. Finally, in four interviews subjects mentioned how they could point the LCD screen at what they were interested in, or that it felt like using a camera, while almost no one mentioned that the LCD display often required them to use two hands or that it was more cumbersome (two cases). 
One strong point of the handheld display was the ease at which people could switch between the real and augmented views. In five of twelve interviews subjects commented on how easy it was to remove the handheld display from their face and swap back to the real world. For example, one subject wroteThe handheld display was easiest to focus and still switch back and forth between the virtual reality image and real sight. Similarly, in four cases people talked about how they liked the handheld display because they could talk to each other facetoface and then just use the display for quick glances at the virtual scene. As one user said: We spent most of our time talking to each other normally, using the device only when necessary. 
In this second experiment we have explored the effect of different types of AR displays on communication behaviors in a colocated collaborative task. In general the AR conditions have produced results that are significantly different that the facetoface case, however when the experiment was conducted the facetoface condition was always run first, followed by the AR conditions in a counterbalanced order. This may have biased some of the facetoface results. However we are more interested in comparing between AR conditions. 
When considering the AR condition results it seems that there is little difference in speech behaviors between the LCD, HHD and HMD conditions. Subjects spoke the same number of words per turn, the same number of turns per second, and the same amount of deictic phrases. However with a handheld display they asked fewer building identification questions than with an LCD panel or HMD. This is because the LCD panel affords only a small field of view, while the HMD removes the user from seeing the world naturally. Both of which make it difficult for a user to see 
the AR buildings or the nonverbal cues of his or her partner. In contrast the handheld display provides the same field of view as the HMD and allows the user to easily see the real world. 
When considering nonverbal communication there is a significant difference in the amount of picking and pointing gestures made between the facetoface condition and the AR conditions. However, there is no difference between AR conditions. Although the users can use both hands in the HMD condition and only one in the other conditions, it appears that the form factor of the AR displays does not greatly affect the ease at which people can make nonverbal gestures. The differences in AR displays do cause the users to stand to compensate for the field of view limitations. Users of the LCD display on average spent over half their time standing while even when using a HMD subjects felt it was necessary to stand for a portion of the time to get a better view of the task space. 
Users did feel strongly about how the difference AR displays allowed them to perceive nonverbal cues and the presence of their partner. From the endoftask surveys users felt that the differences in displays affordances did not affect their ability to interact with the AR content or work together, but did affect how well they could tell where their partner was looking or pointing. The results matched the amount of peripheral cues supported by the displays; the more the user could see an unmediated view of the real world (as with the LCD panel), the more they felt aware of their partner. 
From these two experiments we can propose several rules of thumb for designing AR interfaces for facetoface collaboration. 
First, interaction with the AR content should be based around Tangible Interaction techniques. By having our virtual models appear attached to physical objects, users could manipulate them as easily as the real models. 
Secondly, a large part of communication is about perception. Mediating the view of the real world will affect a users ability to perceive their collaborator and the transmission of nonverbal cues. This may change the language spoken as users try to compensate for limited visual perception. For example, in our experiments users asked more questions to identify the virtual buildings. 
Finally, until HMDs have been developed with a wide field of view and high resolution, and if there is no compelling reason to use an HMD, a display form factor should be chosen that enables a user to easily perceive the unmediated real world. In our case, subjects felt that the LCD and handheld display were both more useful for collaboration than the HMD. Please note that the HMD we used was video seethrough; an optical seethrough HMD will probably produce different results because users can see the real world directly through the HMD regardless of its resolution. 
6 CONCLUSIONS In these two experiments we have begun to explore communication and task behaviors with a facetoface collaborative AR interface. We have found that ease of interaction and some task performance can be unchanged when using different AR displays, but the AR interface does affect the perception of communication cues. 
However there is still a large amount of work that needs to be done to compare how collaboration with these types of interfaces is different from more traditional computer supported tools. We should remind the readers that our results have just been collected from one type of task, objectbased collaboration. It is most likely that results will differ for other tasks. In particular, we expect that conversational tasks such as negotiation will be even more susceptible to differences in the AR interface. Past work on teleconferencing has found that negotiation and conversational tasks are even more sensitive to differences between communication media. We need to conduct experiments where object manipulation is just a small part of the task at hand. 
In our work not only did we reduce perceptual cues by removing peripheral vision, but the AR displays were also showing a bioccular rather than stereoscopic view of the real world. Since this task involved closein object manipulation the lack of stereo would have made it more difficult to pick us and move objects. This may also explain why users across the table appeared to be flat and more distant. We need to conduct another experiment where we compare display properties such as stereo versus nonstereo and video seethrough versus optical seethrough. 
Finally, in the future we intend to explore how AR interfaces can be combined with projective technology and normal desktop displays to enhance facetoface collaboration. One of the greatest benefits of Augmented Reality interfaces is that they can be seamlessly combined with other more traditional interface technology and existing workplace displays. However there are few researchers who have presented interfaces that combine AR techniques with other approaches and these is a large number of interface questions that need to be answered before truly useful interfaces can be developed. 
7 ACKNOWLEDGEMENTS This work was supported in part by the Advanced Research and Development Activity (ARDA) under contract number NMA20101C0006. 
[Anderson 96] Anderson, A. , Newlands, A. , Mullin, J. , Fleming, A. , DohertySneddon, G. , Velden, J. vander (1996) Impact of videomediated communication on simulated service encounters. Interacting with Computers, Vol. 8, no. 2, pp. 193206. 
[Bekker 95] Bekker, M.M., Olson, J.S., and Olson G.M. (1995) Analysis of gestures in facetoface design teams provides guidance for how to use groupware in design, In Proceedings of the Symposium on Designing Interactive Systems, (Ann Arbor, USA, August 2325), 157166. 
[Bimber 2001] Bimber, O., Frohlich, B., Schmalstieg, D., and Encarnacao, L.M. The Virtual Showcase. IEEE Computer Graphics &amp; Applications, vol. 21, no.6, pp. 4855, 2001 
[Blundell 94] Blundell, B.G., and Schwarz, A.J. A Graphics Hierarchy for the Visualization of 3D Images by Means of a Volumetric Display System. In Proceedings of the IEEE Region 10s Ninth Annual International Conference, Singapore, Aug. 2226, 1994, pp. 15. Vol. 1. IEEE, New York. 
[Boyle 94] Boyle, E., Anderson, A., Newlands, A. (1994) The effects of eye contact on dialogue and performance in a cooperative problem solving task. Language and Speech, 37 (1), pp. 120. 
[Chapanis 75] [Chapanis 75] Chapanis, A. (1975) Interactive Human Communication. Scientific American, Vol. 232, pp. 3642, 1975. 
[CruzNeira 92] CruzNeira, C., Sandin, D. J., Defanti, T. A., Kentyon, R. V., and Hart, J. C. The CAVE: Audio Visual Experience Automatic Virtual Environment. Communications of the ACM, 1992, Vol. 35 (6), pp. 65. 
[DalyJones 98] DalyJones, O., Monk, A., Watts, L. Some Advantages of Video Conferencing Over Highquality Audio Conferencing: Fluency and Awareness of Attentional Focus. Int. J. HumanComputer Studies, 1998, 49, 2158. 
[Ekman 69] Ekman, P, and Friesen, W.V. (1969) The repertoire of nonverbal behaviour: categories, origins, usage and coding, Semiotica, 1, 4998. 
[Elrod 92] Elrod, S., et. al. LiveBoard: A large interactive display supporting group meetings, presentations, and remote collaboration. In Proceedings of CHI 92: Human Factors in Computing Systems, Monterey, CA, 1992, pp. 599607. 
[Fjeld 99] Fjeld, M., Voorhorst, F., Bichsel, M., Lauche, K., Rauterberg, M., H., K. (1999). Exploring BrickBased Navigation and Composition in an Augmented Reality. In Proceedings of HUC 99, pp. 102116. 
[Isaacs 93] Isaacs, E., Tang, J. (1993) What video can and cant do for collaboration: A case study. In Proceedings of the ACM Multimedia 93 Conference, pp. 199206, Anaheim, CA. 
[Kitamura 2001] Kitamura, Y., Konishi, T., Yamamoto, S., and Kishino, F. Interactive Stereoscopic Display for Three or More Users, In ACM SIGGRAPH 2001 (Computer Graphics, Annual Conference Series), pp.231239, 2001 
[Kiyokawa 99] Kiyokawa, K, Takemura, H., Yokoya, N., "A Collaboration Supporting Technique by Integrating a Shared Virtual Reality and a Shared Augmented Reality", Proceedings of the IEEE International Conference on Systems, Man and Cybernetics (SMC '99), Vol.VI, pp.4853, Tokyo, 1999. 
[Krauss 67] Krauss R, Bricker P, (1967) Effects of transmission delay and access delay on the efficiency of verbal communication. J. Acoustic Soc. Amer., vol. 41.2, pp. 286292 
[Kruger 95] Kruger, W., Bohn, C., Frohlich, B., Schuth, H., Strauss, W., Wesche, G. The Responsive Workbench: A Virtual Work Environment. IEEE Computer, Vol. 28(7), 1995, pp.4248. 
[LiShu 94] LiShu, Flowers, W. Teledesign: Groupware User Experiments in ThreeDimensional Computer Aided Design. Collaborative Computing, Vol. 1(1), 1994, pp. 114. 
[McCarthy 94] McCarthy, J., Monk, A. Measuring the Quality of ComputerMediated Communication. Behaviour &amp; Information Technology, 1994, Vol. 13, No. 5, 311319. 
[McNeill 92] McNeill, D (1992) Hand and mind: What gestures reveal about thought. Chicago: The university of Chicago Press. 
[Minneman 96] Minneman, S., Harrison, S (1996) A Bike in Hand: a Study of 3D Objects in Design. In Analyzing Design Activity, N. Cross, H., Christiaans, K. Dorst (Eds), J. Wiley, Chichester, 1996. 
[Monk 96] Monk, A.F., McCarthy, J., Watts, L. &amp; DalyJones, O., (1996) Measures of process, in Thomas, P.J. (ed) CSCW requirements and evaluation, Berlin: SpringerVerlag, pp. 125139. 
[Nyerges 98] Nyerges, T., Moore, T., Montejano, R., Compton, M. (1998) Developing and Using Interaction Coding Systems for Studying Groupware Use. HumanComputer Interaction, 1998, Vol. 13, pp. 127165. 
[Ochsman 74] Ochsman R.B., and Chapanis A. The Effects of 10 Communication Modes on the Behaviour of Teams During Cooperative ProblemSolving. International Journal of ManMachine Studies, 6, 1974, 579619. 
[OConaill 97] O'Conaill, B., and Whittaker, S. (1997). Characterizing, predicting and measuring videomediated communication: a conversational approach. In In K. Finn, A. Sellen, S. Wilbur (Eds.), Video mediated communication. LEA: NJ. 
[Ohshima 98] Ohshima, T., Satoh, K., Yamamoto, H., and Tamura, H. (1998) AR2 Hockey system: A collaborative mixed reality system. Trans. VRSJ, vol.3, no.2, pp.5560, 1998. 
[O'Malley 96] O'Malley, C. , Langton, S. , Anderson, A. , DohertySneddon, G. , Bruce, V. (1996) A comparison of facetoface and videomediated interaction. Interacting with Computers, Vol. 8, no. 2, pp. 177192. 
[Pedersen 93] Pedersen, E.R., McCall, K., Moran, T.P., Halasz, F.G. (1993). Tivoli: An Electronic Whiteboard for Informal Workgroup Meetings. In Proceedings of Human Factors in Computing Systems (InterCHI 93) ACM Press, pp. 391398. 
[Rutter 81] Rutter, D., Stephenson, G., Dewey, M. (1981). Visual Communication and the content and style of conversation. British Journal of Social and Clinical Psychology, 17, 1721. 
[Schmalsteig 96] Schmalsteig, D., Fuhrmann, A., Szalavari, Z., Gervautz, M., (1996) StudierstubeAn Environment for Collaboration in Augmented Reality. In CVE 96 Workshop Proceedings, 1920th September 1996, Nottingham, Great Britain. 
[Sellen 95] Sellen, A. Remote Conversations: The effects of mediating talk with technology. Human Computer Interaction, 1995, Vol. 10, No. 4, pp. 401444. 
[Soltan 95] Soltan, P., Trias, J., Dahlke, W., Lasher, M., McDonald, M. LaserBased 3D Volumetric Display System: Second Generation. In Interactive Technology and the New Paradigm for Technology, IOP Press, 1995, pp. 349358. 
[Stefik 87] Stefik, M., Foster, G., Bobrow, D., Kahn, K., Lanning, S., Suchman, L. (1987) Beyond the Chalkboard: Computer Support for Collaboration and Problem Solving in Meetings. In Communications of the ACM, January 1987, Vol 30, no. 1, pp. 3247. 
[Strietz 99] Streitz, N.A., Geiler, J., Holmer, T., Konomi, S., MllerTomfelde, C., Reischl, W., Rexroth, P., Seitz, P., and Steinmetz, R. iLAND: An interactive Landscape for Creativity and Innovation. In ACM Conference on Human Factors in Computing Systems (CHI '99), Pittsburgh, Pennsylvania, U.S.A., May 1520, 1999. ACM Press, New York, 1999, pp. 120127. 
[Tang 92] Tang, J., Isaacs, E. (1992) Why Do Users Like Video? Studies of MultimediaSupported Collaboration. Sun Microsystems Laboratories, Technical Report, SMLI TR925, December 1992. 
[Ullmer 97] Ullmer, B. and Ishii, H., The metaDESK: Models and Prototypes for Tangible User Interfaces. In Proceedings of Symposium on User Interface Software and Technology (UIST '97), (Banff, Alberta, Canada, October, 1997), ACM Press, pp. 223232. 
[Whittaker 97] Whittaker, S., OConnaill, B. The Role of Vision in FacetoFace and Mediated Communication. In VideoMediated Communication, Eds. Finn, K., Sellen, A., Wilbur, S. Lawerance Erlbaum Associates, New Jersey, 1997, pp. 2349. [Williams 77] Williams, E., Experimental Comparison of FacetoFace and Mediated Communication: A Review. Psychological Bulletin, 84, (5), 1977, 963976. 
