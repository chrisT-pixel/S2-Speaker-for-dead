 ATR International University of Washington Hiroshima City University Hikaridai, Seika, Sourakugun Box 352142 341 OzukaHigashi, Asaminamiku Kyoto 61902, Japan Seattle, WA, 98195 USA Hiroshima 7313194, Japan poup@mic.atr.co.jp grof@hitl.washington.edu kato@sys.im.hiroshimacu.ac.jp 
Abstract In the Shared Space project, we explore, innovate, design and evaluate future computing environments that will radically enhance interaction between human and computers as well as interaction between humans mediated by computers. In particular, we investigate how augmented reality enhanced by physical and spatial 3D user interfaces can be used to develop effective facetoface collaborative computing environments. How will we interact in such collaborative spaces? How will we interact with each other? What new applications can be developed using this technology? These are the questions that we are trying to answer in research on Shared Space. This paper provides a short overview of Shared Space, its directions, technologies and applications. 
Keywords: augmented reality, physical interaction, computer vision tracking, collaboration, entertainment. 
In the Shared Space project, we explore, innovate, design and evaluate future computing environments that will radically enhance interaction between human and computers as well as interaction between humans mediated by computers. In particular, we investigate how augmented reality enhanced by physical and spatial 3D user interfaces can be used to develop effective facetoface collaborative computing environments. 
Augmented reality. Augmented reality (AR), i.e. overlaying of virtual objects on the real world, allows us to integrate computergenerated and computercontrolled objects into everyday physical reality [6]. Unlike virtual reality where the physical world is completely replaced with synthetic environments, in augmented reality environments, 3D computer graphics objects are mixed with physical objects to become part of the real world. 
Collaborative computing. Using computers can be a lonely experience: normally, there is no support for collaborative activities in which several people can work together. In real world collaboration objects and information can be simultaneously and asynchronously accessed by multiple participants, with communication discourse flowing freely between the participants. Shared Space aims to allow for a similar freedom of collaborative interaction that we have in physical environments. We also aim to address some of the limita
tions of current collaborative interfaces including the lack of spatial cues, the difficulty of interacting with shared 3D data, the introduction of artificial seams into a collaboration, and the need to be physically present at a computer to collaborate [8, 12]. 
Physical interfaces. Interaction with todays graphical user interfaces (GUIs) is often dubbed as direct, meaning that the user picks and manipulates interface objects using a mouse similarly to how we actually pick and manipulate physical objects. When compared to early command line interfaces, interaction in current GUIs is indeed more direct, nevertheless it can only be loosely compared to our interaction with the physical world. In fact, interface objects do not have physical properties, and picking and manipulating them are simply metaphors that help us understand how to use the interface by drawing from our everyday experiences. Shared Space investigates the use of physical, tangible interfaces [9] where the user can control the computer by physically manipulating multiple simple physical objects that become a part of the user interface. 
Spatial 3D user interfaces. 3D user interfaces, an important topic in virtual reality, explore how users can efficiently and effectively interact in spatial 3D computergenerated environments. In spatial interfaces as well as in the physical world, users are not constrained by the 2D metaphor of conventional desktop user interfaces but can interact freely in space. Shared Space is a 3D user interface that provides the user with rich spatial cues and combines spatial and physical interaction for easy control and manipulation of virtual objects. 
Computer vision tracking and registration. Computer vision techniques have recently become very popular in user interface research [7] Shared Space makes heavy use of computer vision techniques for tracking and registering virtual objects in the physical world [2]. 
The rest of this paper is organized as follows. In the next section, we briefly discuss related work, followed by a more detailed discussion of the technologies involved in Shared Space: augmentation, collaboration, interaction and their implementation based on computer vision tracking and registration techniques. We then describe a collaborative application that uses these technologies, a game demonstrated at SIGGRAPH 99. 
Shared Space has been inspired by a number of previous research projects in augmented reality and ubiqui
tous computing , computer supported collaborative work (CSCW), 3D user interfaces and virtual reality, and tangible and physical computing [15]. Our research on Shared Space integrates many of these individual components into an effective interface that can support intuitive face to face 3D CSCW. 
While the use of spatial cues and threedimensional object manipulation are common in face to face communication, tools for threedimensional CSCW are still rare. One approach is to add collaborative capabilities to existing desktopbased threedimensional packages. However, a twodimensional (2D) interface for threedimensional collaboration can have severe limitations, such as users finding it difficult to visualize depth cues or the different viewpoints of their collaborators [10]. 
Alternative techniques include using large stereo projection screens to project a threedimensional virtual image into space, such as in the CAVE system [5]. Unfortunately, images can only be rendered from a single users viewpoint in this setting, so only one person will see true stereo. While this might be satisfactory for some tasks, such as collaborative viewing, effective face to face CSCW using CAVE is impossible. 
Multiuser immersive virtual environments provide an extremely natural medium for three dimensional CSCW. Research on the DIVE project [4], GreenSpace [11] and other fully immersive multiparticipant virtual environments has shown that collaborative work is indeed intuitive in such surroundings. Participants can seamlessly exchange and communicate gesture, voice and graphical information. However, most current multiuser VR systems are fully immersive, separating the user from the real world: notes, documents, tools and other artifacts of everyday life cannot be easily accessed from immersive virtual environments. 
Unlike other methods for threedimensional CSCW, augmented reality interfaces can overlay graphics and audio onto the real world. This allows for creation of AR interfaces that combine the advantages of virtual environments and possibilities for seamless interaction with real world objects and other collaborators. 
Single user AR interfaces have been developed for computer aided instruction [6], medical visualization [1], information displays and other purposes. These applications have shown that AR interfaces can enable a person to interact with the real world in ways never before possible. However, although AR techniques have proven valuable in single user applications, there has been significantly less research on collaborative, multiuser applications. The AR2 Hockey [12] and the Studierstube project [14] are two of the few exceptions. 
On the interface side while the physical and tangible interfaces have been extensively explored [9], there have been few efforts at combining them with spatial 3D interfaces. Finally, computer vision techniques have been extensively used to track and register virtual objects in 
augmented reality applications. Our approach was inspired by the work of Rekimoto who developed a technique for robust tracking of 2D markers [13]. 
This section discusses key aspects of Shared Space, i.e., augmentation, collaboration, interaction, and implementation based on computer vision tracking and registration techniques. 
Shared Space uses a headmounted display (HMD) with a lightweight camera mounted in front of the display. The output from the camera is connected to a computer and then to the HMD so that the user sees the real world through the video image. In the physical environment there are a number of marked cards with square fiducial patterns on them and a unique identifying symbol in the middle of the pattern. When the user looks at these cards, computer vision techniques are used to identify the specific marker, calculate head position and orientation relative to the fiducial marks, and display 3D virtual images so that they appear precisely registered with the physical objects (Figure 1). The details of the implementation are briefly described later in the paper, for a full description see [2]. 
Shared Space allows users to refer to physical notes, diagrams, books and other real objects while at the same time viewing and interacting with virtual images. More importantly, colocated users can see each other's facial expressions, gestures and body language thus supporting natural facetoface communication cues. Thus the Shared Space interface allows multiple users in the same location to simultaneously work in both the real and virtual world (Figure 2). Since all users share the same database of virtual objects, they see the same virtual objects attached to the markers from their own viewpoints. Users can pick up and show cards to the other participants, or pass or request virtual objects in the same manner that we do with real objects. 
Figure 1: The HMD and a camera are used for registering and viewing virtual objects. Here a samurai model 
Shared Space explores the use of spatial and physical interaction in augmented environments: the user can directly manipulate virtual objects by manipulating marked physical objects with virtual objects on them (Figure 1). The system can robustly track the motion of the physical markers and keep the virtual object precisely aligned relative to the marker. Several markers can be tracked simultaneously so the relative positions of marked objects to each other can be used to trigger virtual object interactions. For example, placing a card with a virtual UFO on it next to one with an alien may trigger an animation of the alien flying in the UFO (Figure 3). There are a wide range of spatial relationships and physical object interactions (shaking, rotating, etc..) that may be used for virtual interactions. 
Shared Space uses a computer vision based tracking algorithm designed by Hirokazu Kato [2]. By tracking rectangular markers of known size the relative camera position and orientation can be found in real time. Once this is known, the virtual camera can be placed at the same position so 3D computer graphics objects appear to be exactly attached to markers (Figure 4). 
A number of applications have been developed and explored using various components and configurations of the Shared Space technology, including a mobile AR conferencing space for remote users [3]. In this section we describe a collaborative entertainment application, which was demonstrated at the Emerging Technologies exhibit at the SIGGRAPH99 conference. The goal of this demonstration was to show how augmented reality could be used to enhance facetoface collaboration in a way that could be used by novices with no training. 
A multiplayer game similar to the game Concentration was designed. We presented visitors with sixteen 5x7 inch playing cards with tracking patterns on one side, and the visitors were required to match cards. The cards were placed on a table that up to three people could gather around. Each user wore a HMD with a camera attached connected to a computer as previously described. When players turned the cards over they saw a different 3D virtual object on each card, such as a witch, horse, alien, or crabs (Figure 3). The goal of the game was to match objects that logically belonged together, such as an alien and UFO. When cards that matched were placed side by side, an animation was triggered involving the objects on the card. For example, when the card with the virtual witch on it was placed next to the card with a virtual broom on it, the witch would jump on the broom and start to fly around in a circle. Sound cues were also played corresponding to the different animations cued. Since the players were all colocated they could easily see each other, and the virtual objects. 
Over the course of the week of August 713 around 3000 conference participants tried the exhibit. Users had no difficulty with the AR interface and exhibited the 
Figure 3: Spatial interaction in Shared Space: users trigger animation of virtual objects (in this case the alien 
Figure 2: In collaborative environment of the Shared Space, users can see and interact with physical and virtual objects, and also see the other participants. 
same sort of collaborative behavior seen in typical facetoface interaction with physical objects. For instance, during players would often spontaneously collaborate with strangers who had a matching card, request and pass cards around as well as collaboratively view objects and completed animations. Furthermore, since the matches were not obvious, users would often request and receive help from other collaborators. 
The physical, tangible nature of our interface made collaborative interaction very easy and intuitive. Users passed cards between each other, picked up and viewed virtual objects from all angles and almost always expressed surprise and enjoyment when they got a match and the static virtual objects came to life. By combining a tangible, physical interface with 3D virtual imagery, we found that even young children could play and enjoy the game (Figure 5). Users did not need to learn any complicated computer interface or command sets the only instructions people needed was to turn the cards over, not cover the tracking patterns, and find objects that matched. 
Users also commented on how much they liked the image recognition and on how little lag there was in the system. This comment is interesting because there was actually a significant (200300ms) delay, however, the users became so immersed they did not notice this. 
In our work on Shared Space, we combine real and virtual worlds to create compelling 3D collaborative experiences in which the technology transparently supports normal human behaviors. It is this transparency that is a key characteristic of the Shared Space research and should enable the continued development of innovative collaborative AR interfaces in the future. In the future we plan on investigating how our tangible augmented reality approach can support seamless transitions between physical reality and immersive virtual reality in a collaborative setting. For more information see: http://www.mic.atr.co.jp/~poup/research/ar/ or http://www.hitl.washington.edu/research/shared_space/ 
We are very thankful to ATR computer graphics artists K. Nakao and J. Kurumisawa who designed models 
and animations for the SIGGRAPH exhibition. We are also grateful to Shigeo Imura for his help in designing and implementing the computer graphics parts of the software platform as well as all HITL and ATR researchers and management, especially Prof. Furness, Dr. Ohya and Dr. Nakatsu for their support of the project, and HITL students and researchers who prepared and worked the SIGGRAPH demonstration. 
Objects with the Real World: Seeing Ultrasound Imagery Within the Patient. Proceedings of SIGGRAPH 92. 1992. ACM. pp. 203210. 
2. Billinghurst, M., Kato, H., Collaborative Mixed Reality. Proceedings of ISMR '99. Springer Verlag. pp. 261284. 
3. Billinghurst, M., Kato, H., Real World Teleconferencing. Proc. of CHI'99, Extended Abstracts. ACM. pp. 194195. 
4. Carlson, C., Hagsand, O., DIVEA Platform for MultiUser Virtual Environments. Computers and Graphics, 1993. 17(6): pp. 663669. 
5. CruzNeira, C., Sandin, D., Defanti, T., Kentyon, R., Hart, J., The CAVE: Audio Visual Experience Automatic Virtual Environment. Communications of the ACM, 1992. 35(6): pp. 65. 
6. Feiner, S., MacIntyre, B., Seligmann, D., KnowledgeBased Augmented Reality. Communications of the ACM, 1993. 36(7): pp. 5362. 
7. Freeman, W., Anderson, D., Beardsley, P., Dodge, C., Roth, M., et al., Computer vision for interactive computer graphics. IEEE Computer Graphics &amp; Applications, 1998. 18(3): pp. 4253. 
8. Ishii, H., Miyake, N., Toward an Open WorkSpace: Computer and Video Fusion Approach of TeamWorkstation. Comm. of the ACM, 1991. 34(12): pp. 3750. 
9. Ishii, H., Ullmer , B., Tangible bits towards seamless interfaces between people, bits and atoms. Proceedings of CHI97. 1997. ACM. pp. 234241. 
10. LiShu, Flowers, W., Teledesign: Groupware User Experiments in ThreeDimensional Computer Aided Design. Collaborative Computing, 1994. 1(1): pp. 114. 
11. Mandeville, J., Davidson, J., Campbell, D., Dahl, A., Schwartz, P., et al., A Shared Virtual Environment for Architectural Design Review. Proceedings of CVE '96 Workshop. 1996. 
12. Ohshima, T., Sato, K., Yamamoto, H., Tamura, H., AR2Hockey: A case study of collaborative augmented reality. Proc. of VRAIS'98. 1998. IEEE. pp. 268295. 
13. Rekimoto, J., Matrix: A Realtime Object Identification and Registration Method for Augmented Reality. Proceedings of Asia Pacific Computer Human Interaction (APCHI'98). 1988. 
14. Schmalsteig, D., Fuhrmann, A., Szalavari, Z., Gervautz, M., StudierstubeAn Environment for Collaboration in Augmented Reality. Proc. of CVE '96 Workshop. 1996. 
15. Weiser, M., Some computer science issues in ubiquitous computing. Comm of ACM, 1993. 36(7): pp. 7584. 
