[{"id": "1311.4528", "submitter": "Aamir Younis Raja", "authors": "R Aamir, A Chernoglazov, C J Bateman, A P H Butler, P H Butler, N G\n  Anderson, S T Bell, R K Panta, J L Healy, J L Mohr, K Rajendran, M F Walsh, N\n  de Ruiter, S P Gieseg, T Woodfield, P F Renaud, L Brooke, S Abdul-Majid, M\n  Clyne, R Glendenning, P J Bones, M Billinghurst, C Bartneck, H Mandalika, R\n  Grasset, N Schleich, N Scott, S J Nik, A Opie, T Janmale, D N Tang, D Kim, R\n  M Doesburg, R Zainon, J P Ronaldson, N J Cook, D J Smithies, K Hodge", "title": "MARS spectral molecular imaging of lamb tissue: data collection and\n  image analysis", "comments": "11 pages, 6 figs", "journal-ref": null, "doi": "10.1088/1748-0221/9/02/P02005", "report-no": null, "categories": "physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral molecular imaging is a new imaging technique able to discriminate\nand quantify different components of tissue simultaneously at high spatial and\nhigh energy resolution. Our MARS scanner is an x-ray based small animal CT\nsystem designed to be used in the diagnostic energy range (20 to 140 keV). In\nthis paper, we demonstrate the use of the MARS scanner, equipped with the\nMedipix3RX spectroscopic photon-processing detector, to discriminate fat,\ncalcium, and water in tissue. We present data collected from a sample of lamb\nmeat including bone as an illustrative example of human tissue imaging. The\ndata is analyzed using our 3D Algebraic Reconstruction Algorithm (MARS-ART) and\nby material decomposition based on a constrained linear least squares\nalgorithm. The results presented here clearly show the quantification of\nlipid-like, water-like and bone-like components of tissue. However, it is also\nclear to us that better algorithms could extract more information of clinical\ninterest from our data. Because we are one of the first to present data from\nmulti-energy photon-processing small animal CT systems, we make the raw,\npartial and fully processed data available with the intention that others can\nanalyze it using their familiar routines. The raw, partially processed and\nfully processed data of lamb tissue along with the phantom calibration data can\nbe found at [http://hdl.handle.net/10092/8531].\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 20:39:18 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2014 04:14:55 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Aamir", "R", ""], ["Chernoglazov", "A", ""], ["Bateman", "C J", ""], ["Butler", "A P H", ""], ["Butler", "P H", ""], ["Anderson", "N G", ""], ["Bell", "S T", ""], ["Panta", "R K", ""], ["Healy", "J L", ""], ["Mohr", "J L", ""], ["Rajendran", "K", ""], ["Walsh", "M F", ""], ["de Ruiter", "N", ""], ["Gieseg", "S P", ""], ["Woodfield", "T", ""], ["Renaud", "P F", ""], ["Brooke", "L", ""], ["Abdul-Majid", "S", ""], ["Clyne", "M", ""], ["Glendenning", "R", ""], ["Bones", "P J", ""], ["Billinghurst", "M", ""], ["Bartneck", "C", ""], ["Mandalika", "H", ""], ["Grasset", "R", ""], ["Schleich", "N", ""], ["Scott", "N", ""], ["Nik", "S J", ""], ["Opie", "A", ""], ["Janmale", "T", ""], ["Tang", "D N", ""], ["Kim", "D", ""], ["Doesburg", "R M", ""], ["Zainon", "R", ""], ["Ronaldson", "J P", ""], ["Cook", "N J", ""], ["Smithies", "D J", ""], ["Hodge", "K", ""]]}, {"id": "1311.5303", "submitter": "Kishore Rajendran", "authors": "K. Rajendran, M. F. Walsh, N. J. A. de Ruiter, A. I. Chernoglazov, R.\n  K. Panta, A. P. H. Butler, P. H. Butler, S. T. Bell, N. G. Anderson, T. B. F.\n  Woodfield, S. J. Tredinnick, J. L. Healy, C. J. Bateman, R. Aamir, R. M. N.\n  Doesburg, P. F. Renaud, S. P. Gieseg, D. J. Smithies, J. L. Mohr, V. B. H.\n  Mandalika, A. M. T. Opie, N. J. Cook, J. P. Ronaldson, S. J. Nik, A.\n  Atharifard, M. Clyne, P. J. Bones, C. Bartneck, R. Grasset, N. Schleich and\n  M. Billinghurst", "title": "Reducing beam hardening effects and metal artefacts using Medipix3RX:\n  With applications from biomaterial science", "comments": null, "journal-ref": null, "doi": "10.1088/1748-0221/9/03/P03015", "report-no": null, "categories": "physics.med-ph physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses methods for reducing beam hardening effects using\nspectral data for biomaterial applications. A small-animal spectral scanner\noperating in the diagnostic energy range was used. We investigate the use of\nphoton-processing features of the Medipix3RX ASIC in reducing beam hardening\nand associated artefacts. A fully operational charge summing mode was used\nduring the imaging routine. We present spectral data collected for metal alloy\nsamples, its analysis using algebraic 3D reconstruction software and volume\nvisualisation using a custom volume rendering software. Narrow high energy\nacquisition using the photon-processing detector revealed substantial reduction\nin beam hardening effects and metal artefacts.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 03:43:15 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Rajendran", "K.", ""], ["Walsh", "M. F.", ""], ["de Ruiter", "N. J. A.", ""], ["Chernoglazov", "A. I.", ""], ["Panta", "R. K.", ""], ["Butler", "A. P. H.", ""], ["Butler", "P. H.", ""], ["Bell", "S. T.", ""], ["Anderson", "N. G.", ""], ["Woodfield", "T. B. F.", ""], ["Tredinnick", "S. J.", ""], ["Healy", "J. L.", ""], ["Bateman", "C. J.", ""], ["Aamir", "R.", ""], ["Doesburg", "R. M. N.", ""], ["Renaud", "P. F.", ""], ["Gieseg", "S. P.", ""], ["Smithies", "D. J.", ""], ["Mohr", "J. L.", ""], ["Mandalika", "V. B. H.", ""], ["Opie", "A. M. T.", ""], ["Cook", "N. J.", ""], ["Ronaldson", "J. P.", ""], ["Nik", "S. J.", ""], ["Atharifard", "A.", ""], ["Clyne", "M.", ""], ["Bones", "P. J.", ""], ["Bartneck", "C.", ""], ["Grasset", "R.", ""], ["Schleich", "N.", ""], ["Billinghurst", "M.", ""]]}, {"id": "2202.06946", "submitter": "Eduardo Ben\\'itez Sandoval PhD", "authors": "Eduardo Benitez Sandoval, Diego Vazquez Rojas, Clarissa A. Parada\n  Cereceres, Alvaro Anzueto Rios, Amit Barde, Mark Billinghurst", "title": "Prototyping a Virtual Agent for Pre-school English Teaching", "comments": "Accepted in the IEEE Virtual Reality Conference 2022, Christchurch,\n  New Zealand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a case study and the insights gained from prototyping an\nIntelligent Virtual Agent (IVA) for English vocabulary building for\nSpanish-speaking preschool children. After an initial exploration to evaluate\nthe feasibility of developing an IVA, we followed a Human-Centered Design (HCD)\napproach to create a prototype. We report on the multidisciplinary process used\nthat incorporated two well-known educative concepts: gamification and\nstory-telling as the main components for engagement. Our results suggest that a\nmultidisciplinary approach to developing an educational IVA is effective. We\nreport on the relevant aspects of the ideation and design processes that\ninformed the vision and mission of the project.\n", "versions": [{"version": "v1", "created": "Tue, 8 Feb 2022 07:26:21 GMT"}], "update_date": "2022-02-16", "authors_parsed": [["Sandoval", "Eduardo Benitez", ""], ["Rojas", "Diego Vazquez", ""], ["Cereceres", "Clarissa A. Parada", ""], ["Rios", "Alvaro Anzueto", ""], ["Barde", "Amit", ""], ["Billinghurst", "Mark", ""]]}, {"id": "2210.07784", "submitter": "Ying Yang", "authors": "Ying Yang, Tim Dwyer, Michael Wybrow, Benjamin Lee, Maxime Cordeil,\n  Mark Billinghurst, Bruce H. Thomas", "title": "Towards Immersive Collaborative Sensemaking", "comments": "Accepted at ACM ISS 2022", "journal-ref": null, "doi": "10.1145/3567741", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When collaborating face-to-face, people commonly use the surfaces and spaces\naround them to perform sensemaking tasks, such as spatially organising\ndocuments, notes or images. However, when people collaborate remotely using\ndesktop interfaces they no longer feel like they are sharing the same space.\nThis limitation may be overcome through collaboration in immersive\nenvironments, which simulate the physical in-person experience. In this paper,\nwe report on a between-groups study comparing collaborations on image\norganisation tasks, in an immersive Virtual Reality (VR) environment to more\nconventional desktop conferencing. Collecting data from 40 subjects in groups\nof four, we measured task performance, user behaviours, collaboration\nengagement and awareness. Overall, the VR and desktop interface resulted in\nsimilar speed, accuracy and social presence rating, but we observed more\nconversations and interaction with objects, and more equal contributions to the\ninteraction from participants within groups in VR. We also identified\ndifferences in coordination and collaborative awareness behaviours between VR\nand desktop platforms. We report on a set of systematic measures for assessing\nVR collaborative experience and a new analysis tool that we have developed to\ncapture user behaviours in collaborative setting. Finally, we provide design\nconsiderations and directions for future work.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2022 13:13:00 GMT"}], "update_date": "2022-10-17", "authors_parsed": [["Yang", "Ying", ""], ["Dwyer", "Tim", ""], ["Wybrow", "Michael", ""], ["Lee", "Benjamin", ""], ["Cordeil", "Maxime", ""], ["Billinghurst", "Mark", ""], ["Thomas", "Bruce H.", ""]]}, {"id": "2301.09402", "submitter": "Ruben Schlagowski", "authors": "Ruben Schlagowski, Dariia Nazarenko, Yekta Can, Kunal Gupta, Silvan\n  Mertes, Mark Billinghurst, Elisabeth Andr\\'e", "title": "Wish You Were Here: Mental and Physiological Effects of Remote Music\n  Collaboration in Mixed Reality", "comments": "Conditionally Accepted for CHI 2023", "journal-ref": null, "doi": "10.1145/3544548.3581162", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With face-to-face music collaboration being severely limited during the\nrecent pandemic, mixed reality technologies and their potential to provide\nmusicians a feeling of \"being there\" with their musical partner can offer\ntremendous opportunities. In order to assess this potential, we conducted a\nlaboratory study in which musicians made music together in real-time while\nsimultaneously seeing their jamming partner's mixed reality point cloud via a\nhead-mounted display and compared mental effects such as flow, affect, and\nco-presence to an audio-only baseline. In addition, we tracked the musicians'\nphysiological signals and evaluated their features during times of\nself-reported flow. For users jamming in mixed reality, we observed a\nsignificant increase in co-presence. Regardless of the condition (mixed reality\nor audio-only), we observed an increase in positive affect after jamming\nremotely. Furthermore, we identified heart rate and HF/LF as promising features\nfor classifying the flow state musicians experienced while making music\ntogether.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2023 12:56:03 GMT"}], "update_date": "2023-01-24", "authors_parsed": [["Schlagowski", "Ruben", ""], ["Nazarenko", "Dariia", ""], ["Can", "Yekta", ""], ["Gupta", "Kunal", ""], ["Mertes", "Silvan", ""], ["Billinghurst", "Mark", ""], ["Andr\u00e9", "Elisabeth", ""]]}, {"id": "2306.03112", "submitter": "Alireza Nia", "authors": "Alireza F. Nia, Vanessa Tang, Gonzalo Maso Talou, Mark Billinghurst", "title": "Synthesizing Affective Neurophysiological Signals Using Generative\n  Models: A Review Paper", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of emotional intelligence in machines is an important step in\nadvancing human-computer interaction. This demands the development of reliable\nend-to-end emotion recognition systems. However, the scarcity of public\naffective datasets presents a challenge. In this literature review, we\nemphasize the use of generative models to address this issue in\nneurophysiological signals, particularly Electroencephalogram (EEG) and\nFunctional Near-Infrared Spectroscopy (fNIRS). We provide a comprehensive\nanalysis of different generative models used in the field, examining their\ninput formulation, deployment strategies, and methodologies for evaluating the\nquality of synthesized data. This review serves as a comprehensive overview,\noffering insights into the advantages, challenges, and promising future\ndirections in the application of generative models in emotion recognition\nsystems. Through this review, we aim to facilitate the progression of\nneurophysiological data augmentation, thereby supporting the development of\nmore efficient and reliable emotion recognition systems.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2023 08:38:30 GMT"}], "update_date": "2023-06-07", "authors_parsed": [["Nia", "Alireza F.", ""], ["Tang", "Vanessa", ""], ["Talou", "Gonzalo Maso", ""], ["Billinghurst", "Mark", ""]]}, {"id": "2306.03381", "submitter": "Elliott Wen", "authors": "Elliott Wen, Chitralekha Gupta, Prasanth Sasikumar, Mark Billinghurst,\n  James Wilmott, Emily Skow, Arindam Dey, Suranga Nanayakkara", "title": "VR.net: A Real-world Dataset for Virtual Reality Motion Sickness\n  Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Researchers have used machine learning approaches to identify motion sickness\nin VR experience. These approaches demand an accurately-labeled, real-world,\nand diverse dataset for high accuracy and generalizability. As a starting point\nto address this need, we introduce `VR.net', a dataset offering approximately\n12-hour gameplay videos from ten real-world games in 10 diverse genres. For\neach video frame, a rich set of motion sickness-related labels, such as\ncamera/object movement, depth field, and motion flow, are accurately assigned.\nBuilding such a dataset is challenging since manual labeling would require an\ninfeasible amount of time. Instead, we utilize a tool to automatically and\nprecisely extract ground truth data from 3D engines' rendering pipelines\nwithout accessing VR games' source code. We illustrate the utility of VR.net\nthrough several applications, such as risk factor detection and sickness level\nprediction. We continuously expand VR.net and envision its next version\noffering 10X more data than the current form. We believe that the scale,\naccuracy, and diversity of VR.net can offer unparalleled opportunities for VR\nmotion sickness research and beyond.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2023 03:43:11 GMT"}], "update_date": "2023-06-07", "authors_parsed": [["Wen", "Elliott", ""], ["Gupta", "Chitralekha", ""], ["Sasikumar", "Prasanth", ""], ["Billinghurst", "Mark", ""], ["Wilmott", "James", ""], ["Skow", "Emily", ""], ["Dey", "Arindam", ""], ["Nanayakkara", "Suranga", ""]]}]